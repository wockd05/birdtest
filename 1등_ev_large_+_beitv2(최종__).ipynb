{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2-sFzaSyp6Ms",
        "outputId": "07c82034-404b-4f44-e179-135543bba244"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "압축 해제가 완료되었습니다: C:/wockd/ultra/ultra/retina/data\n"
          ]
        }
      ],
      "source": [
        "import zipfile\n",
        "import os\n",
        "\n",
        "# 압축 파일 경로와 압축 해제 경로 설정\n",
        "zip_file_path = r'C:/wockd/ultra/ultra/retina/open.zip'  # 압축 파일 경로\n",
        "extract_to_path = r'C:/wockd/ultra/ultra/retina/data'     # 압축 해제 경로\n",
        "\n",
        "# 압축 해제 경로 폴더가 없으면 생성\n",
        "if not os.path.exists(extract_to_path):\n",
        "    os.makedirs(extract_to_path)\n",
        "\n",
        "# 압축 파일 해제\n",
        "try:\n",
        "    with zipfile.ZipFile(zip_file_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(extract_to_path)\n",
        "    print(f\"압축 해제가 완료되었습니다: {extract_to_path}\")\n",
        "except zipfile.BadZipFile:\n",
        "    print(\"압축 파일이 손상되었습니다.\")\n",
        "except Exception as e:\n",
        "    print(f\"압축 해제 중 오류 발생: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k95fPjnRSjV0",
        "outputId": "a6892e1e-4b5b-4dc0-afad-3712fde471a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\transformers\\utils\\generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
            "  torch.utils._pytree._register_pytree_node(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 13:06:22 [INFO] program started\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "\n",
        "from glob import glob\n",
        "from tqdm.auto import tqdm\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from torchvision.transforms import v2\n",
        "import albumentations as A\n",
        "import cv2\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from transformers import Swinv2Model, ConvNextV2Model, AutoModel\n",
        "import timm\n",
        "from PIL import Image\n",
        "\n",
        "torch.set_float32_matmul_precision('high')  # or 'medium' | 'high'\n",
        "# os.environ['WANDB_API_KEY']='xxxxx'\n",
        "# os.environ['WANDB_MODE']='online'\n",
        "# os.environ['WANDB_PROJECT']='basslibrary240210'\n",
        "os.environ['WANDB_MODE']='offline'\n",
        "\n",
        "######## logger ########\n",
        "import sys, logging, IPython\n",
        "logger = logging.getLogger()\n",
        "logging.basicConfig( handlers=[ logging.StreamHandler(stream=sys.stdout), logging.handlers.RotatingFileHandler(filename='run.log', mode='a', maxBytes=512000, backupCount=4) ] )\n",
        "logging_fomatter = logging.Formatter( '%(asctime)s [%(levelname)-4.4s] %(message)s', datefmt='%m/%d %H:%M:%S' )\n",
        "_ = [ h.setFormatter(logging_fomatter) for h in logger.handlers ]\n",
        "logger.setLevel(logging.INFO)\n",
        "def showtraceback(self, *args, **kwargs):\n",
        "    logger.exception('-------Exception----------')\n",
        "IPython.core.interactiveshell.InteractiveShell.showtraceback = showtraceback\n",
        "logger.info('program started')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWhdGH8dQMNa",
        "outputId": "1b1d63f6-6d42-4c19-d88d-be0587232ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0    ./train/TRAIN_00000.jpg\n",
            "1    ./train/TRAIN_00001.jpg\n",
            "2    ./train/TRAIN_00002.jpg\n",
            "3    ./train/TRAIN_00003.jpg\n",
            "4    ./train/TRAIN_00004.jpg\n",
            "Name: img_path, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(train_df['img_path'].head())  # 이미지 경로 열 이름이 'img_path'라고 가정\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4qX0V8CSjTB",
        "outputId": "2851f37d-00bc-4e1a-9742-ced37b67ea67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 13:06:44 [INFO] {'SEED': 42, 'N_SPLIT': 3, 'LABEL_SMOOTHING': 0.05, 'OPTIMIZER': 'AdamW', 'INTERPOLATION': 'robidouxsharp', 'PRECISION': '16', 'MODEL_NAME': 'timm/deit3_small_patch16_224.fb_in22k_ft_in1k', 'IMG_SIZE': 224, 'BATCH_SIZE': 8, 'LR': [7.071067811865476e-06, 1e-07], 'IMG_TRAIN_SIZE': 224}\n"
          ]
        }
      ],
      "source": [
        "CFG = {}\n",
        "CFG['SEED'] = 42\n",
        "CFG['N_SPLIT'] = 3\n",
        "CFG['LABEL_SMOOTHING'] = 0.05\n",
        "CFG['OPTIMIZER'] = 'AdamW'\n",
        "CFG['INTERPOLATION'] = 'robidouxsharp'\n",
        "CFG['PRECISION'] = '16'\n",
        "# #----------------------------------\n",
        "# # [9842]\n",
        "#CFG['MODEL_NAME'] = \"timm/eva_large_patch14_336.in22k_ft_in22k_in1k\"\n",
        "#CFG['IMG_SIZE'] = 336\n",
        "##CFG['BATCH_SIZE'] = 16 ## 16//16G\n",
        "#CFG['LR'] = [ 0.25e-5 * np.sqrt(CFG['BATCH_SIZE']), 1e-7 ]\n",
        "# # ----------------------------------\n",
        "# # [0. --[9836][9842x2]\n",
        "# [0.9811,0.9825,0.9811,0.9819,0.9837]\n",
        "# CFG['MODEL_NAME'] = \"timm/eva_large_patch14_196.in22k_ft_in22k_in1k\"\n",
        "# CFG['IMG_SIZE'] = 196\n",
        "# # CFG['IMG_TRAIN_SIZE'] = 196 * 2\n",
        "# CFG['BATCH_SIZE'] = 48 ## 48/16G(ema), 16/8G\n",
        "# CFG['LR'] = [ 0.25e-5 * np.sqrt(CFG['BATCH_SIZE']), 1e-6 ]\n",
        "# # ----------------------------------\n",
        "# [0.9768, 0.9707, 0.9714, 0.9791, 0.9724 ]\n",
        "# CFG['MODEL_NAME'] = \"timm/convnextv2_large.fcmae_ft_in22k_in1k\" ## 288\n",
        "# # CFG['MODEL_NAME'] = \"facebook/convnextv2-large-22k-224\"\n",
        "# CFG['IMG_SIZE'] = 288\n",
        "# CFG['BATCH_SIZE'] = 16  # 6/8G, 16/16G\n",
        "# CFG['PRECISION'] = '16'\n",
        "# CFG['LR'] = [ 0.25e-5 * np.sqrt(CFG['BATCH_SIZE']), 1e-7 ]\n",
        "# #----------------------------------\n",
        "# ## best_score=0.9699\n",
        "# ## A4000: [9737]\n",
        "# CFG['MODEL_NAME'] = \"timm/swinv2_large_window12_192.ms_in22k\"\n",
        "# CFG['IMG_SIZE'] = 192\n",
        "# CFG['BATCH_SIZE'] = 40 ## 40/16\n",
        "# CFG['LR'] = 0.25e-5 * np.sqrt(CFG['BATCH_SIZE'])\n",
        "# # #----------------------------------\n",
        "# # best_score=0.9805\n",
        "# [0.9818,0.9815,0.9803,0.9825,0.9813]\n",
        "# CFG['MODEL_NAME'] = \"timm/beitv2_large_patch16_224.in1k_ft_in22k_in1k\"\n",
        "# CFG['IMG_SIZE'] = 224\n",
        "# CFG['BATCH_SIZE'] = 48 ## 48//16G(ema), 14//8G memory..\n",
        "# CFG['LR'] = [ 0.25e-5 * np.sqrt(CFG['BATCH_SIZE']), 1e-6 ]\n",
        "# #----------------------------------\n",
        "# [0.9742, ]\n",
        "CFG['MODEL_NAME'] = \"timm/deit3_small_patch16_224.fb_in22k_ft_in1k\"    ## 304MB\n",
        "CFG['IMG_SIZE'] = 224\n",
        "CFG['BATCH_SIZE'] = 8 ## 48//16G, 4//8G memory..\n",
        "CFG['LR'] = [ 0.25e-5 * np.sqrt(CFG['BATCH_SIZE']), 1e-7 ]\n",
        "# #----------------------------------\n",
        "\n",
        "######################################\n",
        "if 'IMG_TRAIN_SIZE' not in CFG:\n",
        "    CFG['IMG_TRAIN_SIZE'] = CFG['IMG_SIZE']\n",
        "logger.info(CFG)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SUrhop8Kp6My",
        "outputId": "9a4866ff-2663-4da9-a8f0-5229723aa8e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 13:06:47 [INFO] cuda\n"
          ]
        }
      ],
      "source": [
        "assert torch.cuda.is_available()\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "torch.set_default_device(device)\n",
        "logger.info(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m2MMtYtgp6My",
        "outputId": "89aae22f-f50b-4941-af9a-a6d21bcf2dc1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 13:06:48 [INFO] seed_everything : 42\n"
          ]
        }
      ],
      "source": [
        "def seed_everything(seed):\n",
        "    logger.info(f'seed_everything : {seed}')\n",
        "\n",
        "    import random, os\n",
        "    import numpy as np\n",
        "    import torch\n",
        "\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "seed_everything(CFG['SEED'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kCL6U72NSxNp"
      },
      "outputs": [],
      "source": [
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, img_path_list, label_list, load_img_size, shuffle=False, transforms=None, interpolation='robidouxsharp' ):\n",
        "        self.df = pd.DataFrame({'img_path_list': img_path_list})\n",
        "        self.interpolation = interpolation\n",
        "        self.load_img_size = load_img_size\n",
        "        logger.info(f'load_img_size={load_img_size}')\n",
        "        if label_list is not None:\n",
        "            self.df['label_list'] = label_list\n",
        "        if shuffle:\n",
        "            self.df = self.df.sample(frac=1.0).reset_index(drop=True)\n",
        "        self.transforms = transforms\n",
        "\n",
        "    # numpy or PIL Image => PIL Image\n",
        "    def get_interpolated_image(self, img, new_image_size):\n",
        "        if self.interpolation == 'pil_lanczos':\n",
        "            if isinstance(img, np.ndarray ):\n",
        "                img = Image.fromarray(img)\n",
        "            return img.resize( (new_image_size, new_image_size), Image.LANCZOS )\n",
        "        elif self.interpolation == 'cv2_lanczos4':\n",
        "            if not isinstance(img, np.ndarray ):\n",
        "                img = np.array(img)\n",
        "            import cv2\n",
        "            img = cv2.cvtColor(img, cv2.COLOR_RGB2BGR)\n",
        "            img = cv2.resize(src, (new_image_size, new_image_size), interpolation=cv2.INTER_LANCZOS4) # 픽셀 크기 지정\n",
        "            img = cv2.cvtColor(np.array(img), cv2.COLOR_BGR2RGB)\n",
        "            return Image.fromarray(img)\n",
        "        else:\n",
        "            if not isinstance(img, np.ndarray ):\n",
        "                img = np.array(img)\n",
        "            from wand import image\n",
        "            with image.Image.from_array(img) as src:\n",
        "                src.resize( new_image_size, new_image_size, filter=self.interpolation )\n",
        "                return Image.fromarray(np.array(src))\n",
        "\n",
        "    # path => PIL Image\n",
        "    def get_image_from_index(self, index, img_size ):\n",
        "        img_path = self.df.img_path_list[index]\n",
        "        fname = img_path.replace('./','').split('.')[0] + '.png'\n",
        "        full_fname = f'img_cached/{img_size}_{self.interpolation}/{fname}'\n",
        "        if os.path.exists(full_fname):\n",
        "            img = Image.open(full_fname)\n",
        "        else:\n",
        "            fname_path = '/'.join(full_fname.split('/')[:-1])\n",
        "            os.makedirs(fname_path, exist_ok = True)\n",
        "            img = self.get_interpolated_image(Image.open(img_path), img_size )\n",
        "            img.save( full_fname )\n",
        "        return img\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        image = self.get_image_from_index(index, self.load_img_size)\n",
        "        if self.transforms is not None:\n",
        "            image = self.transforms(image)\n",
        "        if 'label_list' in self.df.columns:\n",
        "            label = torch.tensor(self.df.label_list[index], dtype=torch.long)  # 라벨 타입 변환\n",
        "            return {'pixel_values': image, 'label': label}\n",
        "        else:\n",
        "            return {'pixel_values': image}\n",
        "\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ixdz7MX0p6M1"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch.optim.lr_scheduler import _LRScheduler\n",
        "\n",
        "class CosineAnnealingWarmupRestarts(_LRScheduler):\n",
        "    \"\"\"\n",
        "    학습률 스케줄링을 수행하는 클래스.\n",
        "    주기적으로 학습률을 코사인 형태로 감소시키며, 초기 워밍업 단계를 지원합니다.\n",
        "\n",
        "    Args:\n",
        "        optimizer (Optimizer): 학습 중인 옵티마이저.\n",
        "        first_cycle_steps (int): 첫 번째 주기의 단계 수.\n",
        "        cycle_mult (float): 다음 주기 단계 수를 증가시키는 배율. 기본값: 1.\n",
        "        max_lr (float): 첫 번째 주기의 최대 학습률. 기본값: 1e-5.\n",
        "        min_lr (float): 최소 학습률. 기본값: 1e-10.\n",
        "        warmup_steps (int): 워밍업 단계 수. 기본값: 0.\n",
        "        gamma (float): 주기별 최대 학습률 감소율. 기본값: 1.\n",
        "        last_epoch (int): 마지막 에폭 인덱스. 기본값: -1.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 optimizer : torch.optim.Optimizer,\n",
        "                 first_cycle_steps : int,\n",
        "                 cycle_mult : float = 1.,\n",
        "                 max_lr : float = 1e-5,\n",
        "                 min_lr : float = 1e-10,\n",
        "                 warmup_steps : int = 0,\n",
        "                 gamma : float = 1.,\n",
        "                 last_epoch : int = -1):\n",
        "        assert warmup_steps < first_cycle_steps, \"Warmup steps must be less than the total steps in the first cycle.\"\n",
        "\n",
        "        # 초기 설정\n",
        "        self.first_cycle_steps = first_cycle_steps  # 첫 번째 주기 단계 수\n",
        "        self.cycle_mult = cycle_mult  # 주기 단계 증가 배율\n",
        "        self.base_max_lr = max_lr  # 초기 최대 학습률\n",
        "        self.max_lr = max_lr  # 현재 주기의 최대 학습률\n",
        "        self.min_lr = min_lr  # 최소 학습률\n",
        "        self.warmup_steps = warmup_steps  # 워밍업 단계 수\n",
        "        self.gamma = gamma  # 주기별 최대 학습률 감소율\n",
        "\n",
        "        self.cur_cycle_steps = first_cycle_steps  # 현재 주기 단계 수\n",
        "        self.cycle = 0  # 주기 횟수\n",
        "        self.step_in_cycle = last_epoch  # 현재 주기의 단계 인덱스\n",
        "\n",
        "        # 부모 클래스 초기화\n",
        "        super(CosineAnnealingWarmupRestarts, self).__init__(optimizer, last_epoch)\n",
        "\n",
        "        # 학습률 초기화 (최소 학습률로 설정)\n",
        "        self.init_lr()\n",
        "\n",
        "    def init_lr(self):\n",
        "        \"\"\"최소 학습률로 초기화.\"\"\"\n",
        "        self.base_lrs = []\n",
        "        for param_group in self.optimizer.param_groups:\n",
        "            param_group['lr'] = self.min_lr\n",
        "            self.base_lrs.append(self.min_lr)\n",
        "\n",
        "    def get_lr(self):\n",
        "        \"\"\"현재 단계에 따른 학습률 계산.\"\"\"\n",
        "        if self.step_in_cycle == -1:\n",
        "            return self.base_lrs  # 초기 상태의 학습률\n",
        "        elif self.step_in_cycle < self.warmup_steps:\n",
        "            # 워밍업 단계: 선형적으로 증가\n",
        "            return [(self.max_lr - base_lr) * self.step_in_cycle / self.warmup_steps + base_lr for base_lr in self.base_lrs]\n",
        "        else:\n",
        "            # 코사인 학습률 감소\n",
        "            return [base_lr + (self.max_lr - base_lr) *\n",
        "                    (1 + math.cos(math.pi * (self.step_in_cycle - self.warmup_steps) /\n",
        "                                  (self.cur_cycle_steps - self.warmup_steps))) / 2\n",
        "                    for base_lr in self.base_lrs]\n",
        "\n",
        "    def step(self, epoch=None):\n",
        "        \"\"\"\n",
        "        스케줄러의 현재 상태를 업데이트하고, 학습률을 재계산.\n",
        "\n",
        "        Args:\n",
        "            epoch (int, optional): 현재 에폭 번호. 기본값: None.\n",
        "        \"\"\"\n",
        "        if epoch is None:\n",
        "            # 에폭이 지정되지 않으면 다음 단계로 이동\n",
        "            epoch = self.last_epoch + 1\n",
        "            self.step_in_cycle += 1\n",
        "            if self.step_in_cycle >= self.cur_cycle_steps:\n",
        "                # 주기가 끝난 경우 다음 주기로 이동\n",
        "                self.cycle += 1\n",
        "                self.step_in_cycle -= self.cur_cycle_steps\n",
        "                self.cur_cycle_steps = int((self.cur_cycle_steps - self.warmup_steps) * self.cycle_mult) + self.warmup_steps\n",
        "        else:\n",
        "            # 특정 에폭으로 설정\n",
        "            if epoch >= self.first_cycle_steps:\n",
        "                if self.cycle_mult == 1.:\n",
        "                    self.step_in_cycle = epoch % self.first_cycle_steps\n",
        "                    self.cycle = epoch // self.first_cycle_steps\n",
        "                else:\n",
        "                    n = int(math.log((epoch / self.first_cycle_steps * (self.cycle_mult - 1) + 1), self.cycle_mult))\n",
        "                    self.cycle = n\n",
        "                    self.step_in_cycle = epoch - int(self.first_cycle_steps * (self.cycle_mult ** n - 1) / (self.cycle_mult - 1))\n",
        "                    self.cur_cycle_steps = self.first_cycle_steps * self.cycle_mult ** n\n",
        "            else:\n",
        "                self.cur_cycle_steps = self.first_cycle_steps\n",
        "                self.step_in_cycle = epoch\n",
        "\n",
        "        # 최대 학습률 감소\n",
        "        self.max_lr = self.base_max_lr * (self.gamma**self.cycle)\n",
        "        self.last_epoch = math.floor(epoch)\n",
        "\n",
        "        # 학습률 업데이트\n",
        "        for param_group, lr in zip(self.optimizer.param_groups, self.get_lr()):\n",
        "            param_group['lr'] = lr\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TfQfZc6lSxJD"
      },
      "outputs": [],
      "source": [
        "class CustomModel(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super(CustomModel, self).__init__()\n",
        "        self.model = model\n",
        "        self.clf = nn.LazyLinear(25)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.model(x)\n",
        "        if not isinstance(x, torch.Tensor):\n",
        "            x = x.pooler_output\n",
        "        if self.clf:\n",
        "            x = self.clf(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VoWIJtWISjOC"
      },
      "outputs": [],
      "source": [
        "train_df = pd.read_csv('C:/wockd/ultra/ultra/retina/train.csv')\n",
        "le = LabelEncoder()\n",
        "train_df['class'] = le.fit_transform(train_df['label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vj2Tb6MuTL5b"
      },
      "outputs": [],
      "source": [
        "if not len(train_df) == len(os.listdir('C:/wockd/ultra/ultra/retina/train')):\n",
        "    raise ValueError()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vrvh81EbQMNd",
        "outputId": "7c605bcf-831a-4c0f-a4d4-2e6511d5be09"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Index(['img_path', 'upscale_img_path', 'label', 'class'], dtype='object')\n",
            "누락된 파일 수: 0\n",
            "Empty DataFrame\n",
            "Columns: [img_path, upscale_img_path, label, class]\n",
            "Index: []\n"
          ]
        }
      ],
      "source": [
        "# 데이터프레임 열 이름 확인\n",
        "print(train_df.columns)\n",
        "\n",
        "# 이미지 경로가 저장된 열 이름이 'img_path'라고 가정\n",
        "missing_files = train_df[~train_df['img_path'].apply(os.path.exists)]\n",
        "print(f\"누락된 파일 수: {len(missing_files)}\")\n",
        "print(missing_files)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETmPNpAqSjLe"
      },
      "outputs": [],
      "source": [
        "skf = StratifiedKFold(n_splits=CFG['N_SPLIT'], random_state=CFG['SEED'], shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jST5bP-jp6M4"
      },
      "outputs": [],
      "source": [
        "image_size = CFG['IMG_SIZE']\n",
        "\n",
        "train_transform_list = [\n",
        "    # v2.RandomHorizontalFlip(), ## eva모델등에서는 성능향상이 없음. 오히려 성능떨어짐.\n",
        "    v2.TrivialAugmentWide(interpolation=v2.InterpolationMode.BICUBIC),\n",
        "    v2.RandomErasing(),\n",
        "    v2.Resize(size=(image_size, image_size), interpolation=v2.InterpolationMode.LANCZOS, antialias=True),\n",
        "    v2.ToImage(), v2.ToDtype( torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "]\n",
        "if CFG['IMG_SIZE'] == CFG['IMG_TRAIN_SIZE']:\n",
        "    train_transform_list = [ a for a in train_transform_list if not isinstance(a, v2.Resize) ]\n",
        "train_transform = v2.Compose(train_transform_list )\n",
        "test_transform = v2.Compose( [\n",
        "    v2.ToImage(), v2.ToDtype( torch.float32, scale=True),\n",
        "    v2.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXVT2UR6QMNd"
      },
      "outputs": [],
      "source": [
        "#체크포인트 없는버전 기존코드\n",
        "from PIL import Image\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "class_weight = torch.FloatTensor(compute_class_weight('balanced', classes=train_df.label.sort_values().unique(), y=train_df.label))\n",
        "\n",
        "def train(model, optimizer, train_loader, val_loader, scheduler, device, validation_steps=0.25, logging_steps=10, use_amp=True):\n",
        "    logger.info(f'{use_amp=}')\n",
        "\n",
        "    model.to(device)\n",
        "    best_score = 0\n",
        "    best_loss = 1000\n",
        "    MAX_PATIENCE = 5\n",
        "    best_patience = MAX_PATIENCE\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weight, label_smoothing=CFG['LABEL_SMOOTHING'], reduction='mean').to(device)\n",
        "    scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    max_steps = len(train_loader)\n",
        "    if not isinstance(validation_steps, int):\n",
        "        validation_steps = int(max_steps * validation_steps)  # 절사\n",
        "    max_steps = (max_steps // validation_steps) * validation_steps\n",
        "\n",
        "    ema_model = None\n",
        "    ema_decay = np.power(np.e, np.log(0.5) / (validation_steps * MAX_PATIENCE))\n",
        "    ema_model = torch.optim.swa_utils.AveragedModel(model, multi_avg_fn=torch.optim.swa_utils.get_ema_multi_avg_fn(ema_decay))\n",
        "\n",
        "    for epoch in range(1, 50):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "        pbar_postfix = {}\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "        for i, batch in enumerate(pbar):\n",
        "            if i >= max_steps:\n",
        "                continue\n",
        "            steps = i + 1\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
        "                    output = model(batch['pixel_values'])\n",
        "                    loss = loss_fn(output, batch['label'].to(torch.int64))  # 라벨 타입 변환\n",
        "\n",
        "                scaler.scale(loss).backward()\n",
        "\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.1)\n",
        "\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "                optimizer.zero_grad()\n",
        "            else:\n",
        "                output = model(batch['pixel_values'])\n",
        "                loss = loss_fn(output, batch['label'].to(torch.int64))  # 라벨 타입 변환\n",
        "\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "            if scheduler is not None:\n",
        "                scheduler.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "            loss = None\n",
        "            output = None\n",
        "            batch = None\n",
        "\n",
        "            if ema_model is not None:\n",
        "                ema_model.update_parameters(model)\n",
        "\n",
        "            if steps % logging_steps == 0:\n",
        "                pbar_postfix.update({\n",
        "                    't_loss0': train_loss[-1],\n",
        "                    'lr': optimizer.param_groups[0][\"lr\"]\n",
        "                })\n",
        "                pbar.set_postfix(pbar_postfix)\n",
        "                run.log({\n",
        "                    \"epoch\": epoch,\n",
        "                    \"step\": steps,\n",
        "                    \"train\": {\"loss\": train_loss[-1]},\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "                }, step=(epoch - 1) * max_steps + steps)\n",
        "\n",
        "            if steps % validation_steps == 0:\n",
        "                _val_loss, _val_score = validation(model, loss_fn, val_loader, device, use_amp)\n",
        "                _train_loss = np.mean(train_loss)\n",
        "\n",
        "                best_score_mark = '*' if best_score < _val_score else ' '\n",
        "                best_loss_mark = '*' if best_loss > _val_loss else ' '\n",
        "                pbar_postfix.update({\n",
        "                    'lr': optimizer.param_groups[0][\"lr\"],\n",
        "                    't_loss': _train_loss,\n",
        "                    'v_loss': _val_loss,\n",
        "                    'v_f1': _val_score\n",
        "                })\n",
        "                pbar.set_postfix(pbar_postfix)\n",
        "                logger.info(f'eps={epoch:d}, lr={optimizer.param_groups[0][\"lr\"]:.3g}, t_loss={_train_loss:.4f}, v_loss={_val_loss:.4f}{best_loss_mark}, v_f1={_val_score:.4f}{best_score_mark}')\n",
        "                run.log({\n",
        "                    \"epoch\": epoch, \"step\": steps,\n",
        "                    \"train\": {\"avg_loss\": _train_loss},\n",
        "                    \"valid\": {\"loss\": _val_loss, \"score\": _val_score},\n",
        "                    \"lr\": optimizer.param_groups[0][\"lr\"]\n",
        "                }, step=(epoch - 1) * max_steps + steps)\n",
        "\n",
        "                if best_score < _val_score:\n",
        "                    best_score = _val_score\n",
        "                    best_patience = MAX_PATIENCE\n",
        "                    best_loss = min(best_loss, _val_loss)\n",
        "                elif best_loss > _val_loss:\n",
        "                    best_loss = _val_loss\n",
        "                    best_patience = MAX_PATIENCE\n",
        "                elif best_patience > 0:\n",
        "                    best_patience -= 1\n",
        "                else:\n",
        "                    logger.info(f'NO_MORE_TRAINING, {best_score=:.4f}')\n",
        "                    if ema_model is not None:\n",
        "                        torch.optim.swa_utils.update_bn(train_loader, ema_model, device)\n",
        "                        ema_val_loss, ema_val_score = validation(ema_model, loss_fn, val_loader, device, use_amp)\n",
        "                        logger.info(f'EMA ::: ema_v_loss={ema_val_loss:.4f}, ema_v_f1={ema_val_score:.4f}')\n",
        "                        run.log({'ema_v_loss': ema_val_loss, 'ema_v_f1': ema_val_score})\n",
        "                    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iBCNkYXzQMNf"
      },
      "outputs": [],
      "source": [
        "#오수정\n",
        "\n",
        "def train(model, optimizer, train_loader, val_loader, scheduler, device, validation_steps=0.25, logging_steps=10, use_amp=True):\n",
        "    logger.info(f'{use_amp=}')\n",
        "    model.to(device)\n",
        "\n",
        "    best_score = 0\n",
        "    best_loss = float('inf')\n",
        "    loss_fn = nn.CrossEntropyLoss(weight=class_weight, label_smoothing=CFG['LABEL_SMOOTHING'], reduction='mean').to(device)\n",
        "    scaler = torch.amp.GradScaler(enabled=use_amp)\n",
        "\n",
        "    max_steps = len(train_loader)\n",
        "    validation_steps = int(max_steps * validation_steps)\n",
        "\n",
        "    for epoch in range(1, 15):\n",
        "        model.train()\n",
        "        train_loss = []\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f'Epoch {epoch}')\n",
        "        for i, batch in enumerate(pbar):\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device, dtype=torch.int64)  # 라벨 타입 변환\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast(device_type='cuda', dtype=torch.float16):  # AMP 활성화\n",
        "                    output = model(inputs)\n",
        "                    loss = loss_fn(output, labels)\n",
        "                scaler.scale(loss).backward()\n",
        "                scaler.step(optimizer)\n",
        "                scaler.update()\n",
        "            else:\n",
        "                output = model(inputs)\n",
        "                loss = loss_fn(output, labels)\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "            train_loss.append(loss.item())\n",
        "            pbar.set_postfix({'t_loss': train_loss[-1]})\n",
        "\n",
        "            # Validation 단계 추가\n",
        "            if (i + 1) % validation_steps == 0:\n",
        "                val_loss, val_score = validation(model, loss_fn, val_loader, device, use_amp)\n",
        "                logger.info(f'Epoch {epoch}, Step {i + 1}, Val Loss: {val_loss:.4f}, Val Score: {val_score:.4f}')\n",
        "\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WxcJXMx1QMNe"
      },
      "outputs": [],
      "source": [
        "# AMP 활성화 여부 설정\n",
        "use_amp = (CFG['PRECISION'] == '16')  # 설정 값이 '16'인 경우 활성화\n",
        "\n",
        "# GradScaler 초기화\n",
        "scaler = torch.amp.GradScaler(enabled=use_amp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lwfCDg0tQMNf"
      },
      "outputs": [],
      "source": [
        "from torch.amp import GradScaler, autocast\n",
        "\n",
        "scaler = GradScaler(enabled=use_amp)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRoj1Hu6p6M6"
      },
      "outputs": [],
      "source": [
        "#기존코드\n",
        "def validation(model, loss_fn, val_loader, device, use_amp):\n",
        "    model = model.to(device)\n",
        "    save_training = model.training\n",
        "    model.eval()\n",
        "\n",
        "    val_loss = []\n",
        "    preds, true_labels = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(val_loader):\n",
        "            true_labels += batch['label'].detach().cpu().numpy().tolist()\n",
        "            with torch.autocast(device_type=device, dtype=torch.float16, enabled=use_amp):\n",
        "                pred = model(batch['pixel_values'])\n",
        "                loss = loss_fn(output, batch['label'].to(torch.int64))  # 라벨 타입 변환\n",
        "\n",
        "            preds += pred.detach().argmax(1).cpu().numpy().tolist()\n",
        "            val_loss.append(loss.item())\n",
        "\n",
        "        _val_loss = np.mean(val_loss)\n",
        "        _val_score = f1_score(true_labels, preds, average='macro')\n",
        "    ## return_to_train..\n",
        "    if save_training:\n",
        "        model.train()\n",
        "    return _val_loss, _val_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "boiJePtAQMNf"
      },
      "outputs": [],
      "source": [
        "#오류 수정\n",
        "def validation(model, loss_fn, val_loader, device, use_amp=True):\n",
        "    model.eval()\n",
        "    val_loss = []\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader:\n",
        "            inputs = batch['pixel_values'].to(device)\n",
        "            labels = batch['label'].to(device, dtype=torch.int64)\n",
        "\n",
        "            if use_amp:\n",
        "                with torch.amp.autocast():\n",
        "                    output = model(inputs)\n",
        "                    loss = loss_fn(output, labels)\n",
        "            else:\n",
        "                output = model(inputs)\n",
        "                loss = loss_fn(output, labels)\n",
        "\n",
        "            val_loss.append(loss.item())\n",
        "            _, predicted = torch.max(output, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    avg_val_loss = sum(val_loss) / len(val_loss)\n",
        "    accuracy = correct / total if total > 0 else 0  # 정확도\n",
        "    return avg_val_loss, accuracy  # 평균 손실과 정확도를 반환\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w1OOndo8p6M6"
      },
      "outputs": [],
      "source": [
        "def prediction(model, test_loader, device):\n",
        "    model = model.to(device)\n",
        "    save_training = model.training\n",
        "    model.eval()\n",
        "    preds = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in tqdm(test_loader):\n",
        "            pixel_values = batch['pixel_values'].to(device)\n",
        "            pred = model(pixel_values)  ## F.softmax(output) ## 의미는 없을 듯.\n",
        "            preds += pred.detach().cpu().numpy().tolist()\n",
        "    if save_training:\n",
        "        model.train()\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eigPpgw-p6M7"
      },
      "outputs": [],
      "source": [
        "def create_model(model_name):\n",
        "    import timm\n",
        "    from transformers import AutoModel, AutoModelForImageClassification, AutoConfig\n",
        "\n",
        "    logger.info(f'create_model: {model_name}')\n",
        "    if '/' not in model_name:\n",
        "        model_name = 'timm/' + model_name\n",
        "\n",
        "    if model_name.startswith('./'):\n",
        "        import nextvit\n",
        "        model = CustomModel(timm.create_model('nextvit_large', pretrained=True))  # checkpoint_path 제거\n",
        "    elif model_name.startswith('facebook/hiera_'):\n",
        "        from hiera import Hiera  ## pip install hiera-transformer\n",
        "        model = CustomModel(Hiera.from_pretrained(model_name))\n",
        "    elif model_name.startswith('timm/'):\n",
        "        model = CustomModel(timm.create_model(model_name, pretrained=True))\n",
        "    else:\n",
        "        model = CustomModel(AutoModel.from_pretrained(model_name))\n",
        "    model.eval()\n",
        "    model(torch.rand((1, 3, CFG['IMG_SIZE'], CFG['IMG_SIZE'])).type(torch.float32))  # initalize_lazyLinear..\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VCsNkLtp6M8"
      },
      "source": [
        "# 모델 훈련"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TTnXL_v5QMNg",
        "outputId": "fc62960f-950a-4ec8-996f-d84b7daa2a77"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 08:26:59 [INFO] fold_idx=0 started\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 08:27:00 [INFO] load_img_size=224\n",
            "11/20 08:27:00 [INFO] load_img_size=224\n",
            "11/20 08:27:00 [INFO] create_model: timm/deit3_large_patch16_224.fb_in22k_ft_in1k\n",
            "11/20 08:27:00 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_large_patch16_224.fb_in22k_ft_in1k)\n",
            "11/20 08:27:00 [INFO] [timm/deit3_large_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
            "11/20 08:27:00 [INFO] use_amp=True\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_5728\\2040297209.py:9: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "Epoch 1:   0%|          | 0/2112 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "##체크포인트 제거\n",
        "from datetime import datetime\n",
        "dt_str = datetime.now().strftime('%m%d%H%M')\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(f'{fold_idx=} started')\n",
        "    import wandb\n",
        "    run = wandb.init(\n",
        "        name=f'fold{fold_idx+1}_{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}_{dt_str}',\n",
        "        config=CFG,\n",
        "        reinit=True)\n",
        "\n",
        "    # 학습 및 검증 데이터 분리\n",
        "    train_fold_df = train_df.loc[train_index, :]\n",
        "    val_fold_df = train_df.loc[val_index, :]\n",
        "\n",
        "    # 학습 데이터셋 및 DataLoader 생성\n",
        "    train_dataset = CustomDataset(\n",
        "        train_fold_df['img_path'].values, train_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "        shuffle=True, transforms=train_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'],\n",
        "        shuffle=True,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=4)\n",
        "\n",
        "    # 검증 데이터셋 및 DataLoader 생성\n",
        "    val_dataset = CustomDataset(\n",
        "        val_fold_df['img_path'].values,\n",
        "        val_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False,\n",
        "        transforms=test_transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] * 2,\n",
        "        shuffle=False,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=4)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model(CFG['MODEL_NAME'])\n",
        "\n",
        "    # 옵티마이저 및 스케줄러 초기화\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CFG['LR'][0],\n",
        "        weight_decay=0.001)  # 논문에서 추천값은 0.001\n",
        "\n",
        "    scheduler = CosineAnnealingWarmupRestarts(\n",
        "        optimizer,\n",
        "        first_cycle_steps=int(len(train_loader)) // 4,\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=CFG['LR'][0] * 2,\n",
        "        min_lr=CFG['LR'][1],\n",
        "        warmup_steps=0,\n",
        "        gamma=0.93)\n",
        "\n",
        "    # 학습 함수 호출\n",
        "    model = train(\n",
        "        model,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        scheduler,\n",
        "        device,\n",
        "        use_amp=(CFG['PRECISION'] == '16')  # AMP(Automatic Mixed Precision) 사용 여부\n",
        "    )\n",
        "\n",
        "    # 메모리 정리\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(f'{fold_idx=} finished')\n",
        "    run.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bbZBKseFQMNg",
        "outputId": "cb309d14-eeb6-46e4-81ce-25a8060756f1"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'skf' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[2], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdatetime\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m datetime\n\u001b[0;32m      3\u001b[0m dt_str \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mH\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mM\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fold_idx, (train_index, val_index) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mskf\u001b[49m\u001b[38;5;241m.\u001b[39msplit(train_df, train_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mclass\u001b[39m\u001b[38;5;124m'\u001b[39m])):\n\u001b[0;32m      6\u001b[0m     gc\u001b[38;5;241m.\u001b[39mcollect()\n\u001b[0;32m      7\u001b[0m     torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mempty_cache()\n",
            "\u001b[1;31mNameError\u001b[0m: name 'skf' is not defined"
          ]
        }
      ],
      "source": [
        "#랜덤 1500개\n",
        "from datetime import datetime\n",
        "dt_str = datetime.now().strftime('%m%d%H%M')\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(f'{fold_idx=} started')\n",
        "    import wandb\n",
        "    run = wandb.init(\n",
        "        name=f'fold{fold_idx+1}_{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}_{dt_str}',\n",
        "        config=CFG,\n",
        "        reinit=True)\n",
        "\n",
        "    # 학습 및 검증 데이터 분리\n",
        "    train_fold_df = train_df.loc[train_index, :]\n",
        "    val_fold_df = train_df.loc[val_index, :]\n",
        "\n",
        "    # 학습 데이터 샘플링\n",
        "    sample_size = min(1500, len(train_fold_df))  # 샘플 크기를 최대 1500으로 제한\n",
        "    train_fold_df = train_fold_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 학습 데이터셋 및 DataLoader 생성\n",
        "    train_dataset = CustomDataset(\n",
        "        train_fold_df['img_path'].values, train_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "        shuffle=True, transforms=train_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] // 2,\n",
        "        shuffle=True,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 검증 데이터셋 및 DataLoader 생성\n",
        "    val_dataset = CustomDataset(\n",
        "        val_fold_df['img_path'].values,\n",
        "        val_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False,\n",
        "        transforms=test_transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] * 2,\n",
        "        shuffle=False,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model(CFG['MODEL_NAME'])\n",
        "\n",
        "    # 옵티마이저 및 스케줄러 초기화\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CFG['LR'][0],\n",
        "        weight_decay=0.001)  # 논문에서 추천값은 0.001\n",
        "\n",
        "    scheduler = CosineAnnealingWarmupRestarts(\n",
        "        optimizer,\n",
        "        first_cycle_steps=int(len(train_loader)) // 4,\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=CFG['LR'][0] * 2,\n",
        "        min_lr=CFG['LR'][1],\n",
        "        warmup_steps=0,\n",
        "        gamma=0.93)\n",
        "\n",
        "    # 학습 함수 호출\n",
        "    model = train(\n",
        "        model,\n",
        "        optimizer,\n",
        "        train_loader,\n",
        "        val_loader,\n",
        "        scheduler,\n",
        "        device,\n",
        "        use_amp=(CFG['PRECISION'] == '16')  # AMP(Automatic Mixed Precision) 사용 여부\n",
        "    )\n",
        "\n",
        "    # 메모리 정리\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(f'{fold_idx=} finished')\n",
        "    run.finish()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9C7_VvOQMNh",
        "outputId": "539865b8-6ba4-467f-e918-812d82c69a4e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 08:47:36 [INFO] fold_idx=0 started\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:d1t4d9t0) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\wockd\\ultra\\ultra\\retina\\wandb\\offline-run-20241120_084156-d1t4d9t0<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\offline-run-20241120_084156-d1t4d9t0\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:d1t4d9t0). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 08:47:38 [INFO] load_img_size=224\n",
            "11/20 08:47:38 [INFO] load_img_size=224\n",
            "11/20 08:47:38 [INFO] create_model: timm/deit3_large_patch16_224.fb_in22k_ft_in1k\n",
            "11/20 08:47:38 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_large_patch16_224.fb_in22k_ft_in1k)\n",
            "11/20 08:47:38 [INFO] [timm/deit3_large_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
            "11/20 08:47:39 [INFO] use_amp=True\n",
            "11/20 08:47:39 [ERRO] -------Exception----------\n",
            "Traceback (most recent call last):\n",
            "  File \"c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3550, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\2282410872.py\", line 72, in <module>\n",
            "    model = train(\n",
            "  File \"C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\4110448684.py\", line 8, in train\n",
            "    loss_fn = nn.CrossEntropyLoss(weight=class_weight, label_smoothing=CFG['LABEL_SMOOTHING'], reduction='mean').to(device)\n",
            "NameError: name 'class_weight' is not defined\n"
          ]
        }
      ],
      "source": [
        "# 이미지 랜덤 1500 및 오류 수정 / 폴드5에 에폭 50 T_loss 1이상\n",
        "from datetime import datetime\n",
        "dt_str = datetime.now().strftime('%m%d%H%M')\n",
        "\n",
        "# K-Fold Cross-Validation 학습 루프\n",
        "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(f'{fold_idx=} started')\n",
        "\n",
        "    # W&B 초기화\n",
        "    import wandb\n",
        "    run = wandb.init(\n",
        "        name=f'fold{fold_idx+1}_{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}_{dt_str}',\n",
        "        config=CFG,\n",
        "        reinit=True)\n",
        "\n",
        "    # 학습 및 검증 데이터 분리\n",
        "    train_fold_df = train_df.loc[train_index, :]\n",
        "    val_fold_df = train_df.loc[val_index, :]\n",
        "\n",
        "    # 학습 데이터 샘플링\n",
        "    sample_size = min(1500, len(train_fold_df))  # 샘플 크기를 최대 1500으로 제한\n",
        "    train_fold_df = train_fold_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 학습 데이터셋 및 DataLoader 생성\n",
        "    train_dataset = CustomDataset(\n",
        "        train_fold_df['img_path'].values, train_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "        shuffle=True, transforms=train_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] // 2,\n",
        "        shuffle=True,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 검증 데이터셋 및 DataLoader 생성\n",
        "    val_dataset = CustomDataset(\n",
        "        val_fold_df['img_path'].values,\n",
        "        val_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False,\n",
        "        transforms=test_transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] * 2,\n",
        "        shuffle=False,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model(CFG['MODEL_NAME']).to(device)\n",
        "\n",
        "    # 옵티마이저 및 스케줄러 초기화\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CFG['LR'][0],\n",
        "        weight_decay=0.001)  # 논문에서 추천값은 0.001\n",
        "\n",
        "    scheduler = CosineAnnealingWarmupRestarts(\n",
        "        optimizer,\n",
        "        first_cycle_steps=int(len(train_loader)) // 4,\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=CFG['LR'][0] * 2,\n",
        "        min_lr=CFG['LR'][1],\n",
        "        warmup_steps=0,\n",
        "        gamma=0.93)\n",
        "\n",
        "    # 학습 함수 호출 (train 함수 사용)\n",
        "    model = train(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        use_amp=(CFG['PRECISION'] == '16')  # AMP 사용 여부\n",
        "    )\n",
        "\n",
        "    # 메모리 정리\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(f'{fold_idx=} finished')\n",
        "    run.finish()\n",
        "\n",
        "#이게 성공한 코드"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GaoKxcRVQMNh",
        "outputId": "5b7d824f-e035-4a39-b5d8-033fa91396dd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:05:06 [INFO] fold_idx=0 started\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Finishing last run (ID:gm5pvc1l) before initializing another..."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\wockd\\ultra\\ultra\\retina\\wandb\\offline-run-20241120_115918-gm5pvc1l<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\offline-run-20241120_115918-gm5pvc1l\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Successfully finished last run (ID:gm5pvc1l). Initializing new run:<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:05:07 [INFO] load_img_size=224\n",
            "11/20 12:05:07 [INFO] load_img_size=224\n",
            "11/20 12:05:07 [INFO] create_model: timm/deit3_small_patch16_224.fb_in22k_ft_in1k\n",
            "11/20 12:05:07 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_small_patch16_224.fb_in22k_ft_in1k)\n",
            "11/20 12:05:08 [INFO] [timm/deit3_small_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 3.4121, Validation Accuracy: 0.0411\n",
            "Epoch 2, Train Loss: 2.2405, Validation Accuracy: 0.8162\n",
            "Epoch 3, Train Loss: 0.8413, Validation Accuracy: 0.8890\n",
            "Epoch 4, Train Loss: 0.5732, Validation Accuracy: 0.9017\n",
            "Epoch 5, Train Loss: 0.4532, Validation Accuracy: 0.9000\n",
            "Epoch 6, Train Loss: 0.4075, Validation Accuracy: 0.9070\n",
            "Epoch 7, Train Loss: 0.3296, Validation Accuracy: 0.9028\n",
            "Epoch 8, Train Loss: 0.3081, Validation Accuracy: 0.9022\n",
            "Epoch 9, Train Loss: 0.2854, Validation Accuracy: 0.9153\n",
            "Epoch 10, Train Loss: 0.2694, Validation Accuracy: 0.9104\n",
            "Epoch 11, Train Loss: 0.2732, Validation Accuracy: 0.9130\n",
            "Epoch 12, Train Loss: 0.2786, Validation Accuracy: 0.9138\n",
            "Epoch 13, Train Loss: 0.2215, Validation Accuracy: 0.9136\n",
            "Epoch 14, Train Loss: 0.2268, Validation Accuracy: 0.9128\n",
            "최적의 모델 저장 (Fold 0, val_score=0.9153)\n",
            "11/20 12:14:21 [INFO] fold_idx=0 finished\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\wockd\\ultra\\ultra\\retina\\wandb\\offline-run-20241120_120506-tf6om2lg<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\offline-run-20241120_120506-tf6om2lg\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:14:22 [INFO] fold_idx=1 started\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:14:23 [INFO] load_img_size=224\n",
            "11/20 12:14:23 [INFO] load_img_size=224\n",
            "11/20 12:14:23 [INFO] create_model: timm/deit3_small_patch16_224.fb_in22k_ft_in1k\n",
            "11/20 12:14:23 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_small_patch16_224.fb_in22k_ft_in1k)\n",
            "11/20 12:14:23 [INFO] [timm/deit3_small_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 3.3984, Validation Accuracy: 0.0279\n",
            "Epoch 2, Train Loss: 2.1984, Validation Accuracy: 0.7880\n",
            "Epoch 3, Train Loss: 0.7971, Validation Accuracy: 0.8909\n",
            "Epoch 4, Train Loss: 0.5852, Validation Accuracy: 0.9005\n",
            "Epoch 5, Train Loss: 0.4862, Validation Accuracy: 0.9041\n",
            "Epoch 6, Train Loss: 0.3531, Validation Accuracy: 0.9055\n",
            "Epoch 7, Train Loss: 0.3921, Validation Accuracy: 0.9123\n",
            "Epoch 8, Train Loss: 0.3191, Validation Accuracy: 0.9028\n",
            "Epoch 9, Train Loss: 0.3120, Validation Accuracy: 0.9117\n",
            "Epoch 10, Train Loss: 0.2799, Validation Accuracy: 0.9197\n",
            "Epoch 11, Train Loss: 0.2843, Validation Accuracy: 0.9170\n",
            "Epoch 12, Train Loss: 0.2441, Validation Accuracy: 0.9193\n",
            "Epoch 13, Train Loss: 0.2358, Validation Accuracy: 0.9199\n",
            "Epoch 14, Train Loss: 0.2239, Validation Accuracy: 0.9178\n",
            "최적의 모델 저장 (Fold 1, val_score=0.9199)\n",
            "11/20 12:25:27 [INFO] fold_idx=1 finished\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\wockd\\ultra\\ultra\\retina\\wandb\\offline-run-20241120_121422-hysm4gk8<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\offline-run-20241120_121422-hysm4gk8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:25:28 [INFO] fold_idx=2 started\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.18.7"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/20 12:25:28 [INFO] load_img_size=224\n",
            "11/20 12:25:28 [INFO] load_img_size=224\n",
            "11/20 12:25:28 [INFO] create_model: timm/deit3_small_patch16_224.fb_in22k_ft_in1k\n",
            "11/20 12:25:28 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_small_patch16_224.fb_in22k_ft_in1k)\n",
            "11/20 12:25:28 [INFO] [timm/deit3_small_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler(enabled=use_amp)\n",
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_25380\\1940753084.py:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast(dtype=torch.float16):\n",
            "c:\\Users\\wockd\\anaconda3\\envs\\newwoc\\lib\\site-packages\\torch\\autograd\\graph.py:825: UserWarning: cuDNN SDPA backward got grad_output.strides() != output.strides(), attempting to materialize a grad_output with matching strides... (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\cudnn\\MHA.cpp:676.)\n",
            "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1, Train Loss: 3.3305, Validation Accuracy: 0.0388\n",
            "Epoch 2, Train Loss: 2.0940, Validation Accuracy: 0.8236\n",
            "Epoch 3, Train Loss: 0.7321, Validation Accuracy: 0.8772\n",
            "Epoch 4, Train Loss: 0.5322, Validation Accuracy: 0.8920\n",
            "Epoch 5, Train Loss: 0.4047, Validation Accuracy: 0.9047\n",
            "Epoch 6, Train Loss: 0.4098, Validation Accuracy: 0.9072\n",
            "Epoch 7, Train Loss: 0.3829, Validation Accuracy: 0.9034\n",
            "Epoch 8, Train Loss: 0.3310, Validation Accuracy: 0.9058\n",
            "Epoch 9, Train Loss: 0.2728, Validation Accuracy: 0.9047\n",
            "Epoch 10, Train Loss: 0.2467, Validation Accuracy: 0.9149\n",
            "Epoch 11, Train Loss: 0.3120, Validation Accuracy: 0.9064\n",
            "Epoch 12, Train Loss: 0.2435, Validation Accuracy: 0.9077\n",
            "Epoch 13, Train Loss: 0.2041, Validation Accuracy: 0.9130\n",
            "Epoch 14, Train Loss: 0.2038, Validation Accuracy: 0.9102\n",
            "11/20 12:36:10 [INFO] fold_idx=2 finished\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "You can sync this run to the cloud by running:<br/><code>wandb sync c:\\wockd\\ultra\\ultra\\retina\\wandb\\offline-run-20241120_122528-upnn9n9v<code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\offline-run-20241120_122528-upnn9n9v\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전체 Fold 중 최적의 모델이 저장되었습니다.\n"
          ]
        }
      ],
      "source": [
        "#최종?\n",
        "from datetime import datetime\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "import gc\n",
        "import wandb\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# 현재 시간 문자열 생성\n",
        "dt_str = datetime.now().strftime('%m%d%H%M')\n",
        "\n",
        "# K-Fold Cross-Validation 학습 루프\n",
        "best_score = 0.0  # 최적의 검증 점수 초기화\n",
        "best_model = None  # 최적의 모델 초기화\n",
        "\n",
        "for fold_idx, (train_index, val_index) in enumerate(skf.split(train_df, train_df['class'])):\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    logger.info(f'{fold_idx=} started')\n",
        "\n",
        "    # W&B 초기화\n",
        "    run = wandb.init(\n",
        "        name=f'fold{fold_idx+1}_{CFG[\"MODEL_NAME\"].split(\"/\")[1].split(\"-\")[0]}_{dt_str}',\n",
        "        config=CFG,\n",
        "        reinit=True)\n",
        "\n",
        "    # 학습 및 검증 데이터 분리\n",
        "    train_fold_df = train_df.loc[train_index, :]\n",
        "    val_fold_df = train_df.loc[val_index, :]\n",
        "\n",
        "    # 학습 데이터 샘플링\n",
        "    sample_size = min(1500, len(train_fold_df))  # 샘플 크기를 최대 1500으로 제한\n",
        "    train_fold_df = train_fold_df.sample(n=sample_size, random_state=42).reset_index(drop=True)\n",
        "\n",
        "    # 클래스 가중치 계산\n",
        "    class_weight = torch.FloatTensor(compute_class_weight(\n",
        "        'balanced',\n",
        "        classes=train_fold_df['class'].unique(),\n",
        "        y=train_fold_df['class']\n",
        "    )).to(device)\n",
        "\n",
        "    # 학습 데이터셋 및 DataLoader 생성\n",
        "    train_dataset = CustomDataset(\n",
        "        train_fold_df['img_path'].values, train_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_TRAIN_SIZE'],\n",
        "        shuffle=True, transforms=train_transform)\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] // 2,\n",
        "        shuffle=True,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 검증 데이터셋 및 DataLoader 생성\n",
        "    val_dataset = CustomDataset(\n",
        "        val_fold_df['img_path'].values,\n",
        "        val_fold_df['class'].values,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False,\n",
        "        transforms=test_transform)\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=CFG['BATCH_SIZE'] * 2,\n",
        "        shuffle=False,\n",
        "        generator=torch.Generator(device=device),\n",
        "        num_workers=0)\n",
        "\n",
        "    # 모델 생성\n",
        "    model = create_model(CFG['MODEL_NAME']).to(device)\n",
        "\n",
        "    # 옵티마이저 및 스케줄러 초기화\n",
        "    optimizer = torch.optim.AdamW(\n",
        "        model.parameters(),\n",
        "        lr=CFG['LR'][0],\n",
        "        weight_decay=0.001  # 논문에서 추천값은 0.001\n",
        "    )\n",
        "\n",
        "    scheduler = CosineAnnealingWarmupRestarts(\n",
        "        optimizer,\n",
        "        first_cycle_steps=int(len(train_loader)) // 4,\n",
        "        cycle_mult=1.0,\n",
        "        max_lr=CFG['LR'][0] * 2,\n",
        "        min_lr=CFG['LR'][1],\n",
        "        warmup_steps=0,\n",
        "        gamma=0.93\n",
        "    )\n",
        "\n",
        "    # 학습 함수 호출\n",
        "    model, val_score = train(\n",
        "        model=model,\n",
        "        optimizer=optimizer,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        scheduler=scheduler,\n",
        "        device=device,\n",
        "        class_weight=class_weight,  # 클래스 가중치 전달\n",
        "        use_amp=(CFG['PRECISION'] == '16')  # AMP 사용 여부\n",
        "    )\n",
        "\n",
        "    # 최적의 모델 저장\n",
        "    if val_score > best_score:\n",
        "        best_score = val_score\n",
        "        best_model = model\n",
        "        torch.save(model.state_dict(), f\"best_model_fold_{fold_idx}.pth\")\n",
        "        print(f\"최적의 모델 저장 (Fold {fold_idx}, val_score={val_score:.4f})\")\n",
        "\n",
        "    # 메모리 정리\n",
        "    model = None\n",
        "    gc.collect()\n",
        "    torch.cuda.empty_cache()\n",
        "    logger.info(f'{fold_idx=} finished')\n",
        "    run.finish()\n",
        "\n",
        "# 가장 성능이 좋은 Fold 저장\n",
        "if best_model is not None:\n",
        "    torch.save(best_model.state_dict(), \"final_best_model.pth\")\n",
        "    print(\"전체 Fold 중 최적의 모델이 저장되었습니다.\")\n",
        "else:\n",
        "    print(\"모델이 저장되지 않았습니다.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLeJOLVKp6M_"
      },
      "source": [
        "# 모델 앙상블 및 추론"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EB61f9szVm5S"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vyMMC_VBVnWg"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QKzBQ6np6M_"
      },
      "outputs": [],
      "source": [
        "test_df = pd.read_csv('C:/wockd/ultra/ultra/retina/test.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7yqQo_drVn-j"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "ckpt_dir = 'C:\\\\wockd\\\\ultra\\\\ultra\\\\retina\\\\ckpt_kfold'\n",
        "if not os.path.exists(ckpt_dir):\n",
        "    os.makedirs(ckpt_dir)\n",
        "    print(f\"Directory '{ckpt_dir}' created.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "62hdvH6iYvKx"
      },
      "outputs": [],
      "source": [
        "#체크포인트 - 사용 xxxx\n",
        "\n",
        "# 체크포인트 파일 리스트 생성\n",
        "ckpt_df = pd.DataFrame({'fname': glob('./ckpt/*.ckpt')})\n",
        "\n",
        "# 문자열 변환 (NaN 처리 포함)\n",
        "ckpt_df['fname'] = ckpt_df['fname'].fillna('').astype(str)\n",
        "\n",
        "# 파일 수정 시간 추가\n",
        "ckpt_df['mtime'] = ckpt_df['fname'].apply(lambda x: int(os.stat(x).st_mtime) if x else 0)\n",
        "\n",
        "# 모델 이름 추출\n",
        "ckpt_df['model_name'] = ckpt_df['fname'].apply(\n",
        "    lambda x: re.search(r'./ckpt/(.*?)-fold', x)[1] if re.search(r'./ckpt/(.*?)-fold', x) else 'unknown')\n",
        "\n",
        "# 이미지 사이즈 추출 (없으면 기본값 0)\n",
        "ckpt_df['img_size'] = ckpt_df['fname'].apply(\n",
        "    lambda x: int(re.search(r'patch[0-9]+_([0-9]+)', x + 'patch0_0')[1]) if re.search(r'patch[0-9]+_([0-9]+)', x) else 0)\n",
        "\n",
        "# EMA 체크\n",
        "ckpt_df['is_ema'] = ckpt_df['fname'].str.endswith('ema.ckpt').astype(int)\n",
        "\n",
        "# fold index 추출\n",
        "ckpt_df['fold_idx'] = ckpt_df['fname'].apply(\n",
        "    lambda x: int(re.search(r'fold_idx=([0-9])-', x)[1]) if re.search(r'fold_idx=([0-9])-', x) else -1)\n",
        "\n",
        "# validation loss 추출 (없으면 기본값 0.0)\n",
        "ckpt_df['val_loss'] = ckpt_df['fname'].apply(\n",
        "    lambda x: float(re.search(r'val_loss=(0\\.[0-9]+)', x)[1]) if re.search(r'val_loss=(0\\.[0-9]+)', x) else 0.0)\n",
        "\n",
        "# validation score 추출 (없으면 기본값 0.0)\n",
        "ckpt_df['val_score'] = ckpt_df['fname'].apply(\n",
        "    lambda x: float(re.search(r'val_score=(0\\.[0-9]+)', x)[1]) if re.search(r'val_score=(0\\.[0-9]+)', x) else 0.0)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gh5gpdAZQMNn",
        "outputId": "d3f60b0b-4a77-421c-a1e5-75b272f5d6cc"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_2368\\2764215821.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n",
            "Predicting: 100%|██████████| 107/107 [00:42<00:00,  2.50it/s]\n",
            "Predicting: 100%|██████████| 107/107 [00:15<00:00,  7.06it/s]\n",
            "Predicting: 100%|██████████| 107/107 [00:15<00:00,  7.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "앙상블 결과가 C:\\wockd\\ultra\\ultra\\retina\\final_predictions.csv에 저장되었습니다.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#폴드 3개 앙상블 오류 수정\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "# 모델 생성 함수\n",
        "def create_model(model_name):\n",
        "    import timm\n",
        "    model = timm.create_model(model_name, pretrained=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# 예측 함수\n",
        "def predict(model, data_loader, device):\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for images in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            preds.append(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# 앙상블 예측 함수\n",
        "def ensemble_predictions(model_paths, model_name, test_loader, device):\n",
        "    ensemble_preds = []\n",
        "    for model_path in model_paths:\n",
        "        # 모델 로드\n",
        "        model = create_model(model_name).to(device)\n",
        "        state_dict = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(state_dict, strict=False)  # strict=False로 불일치 허용\n",
        "\n",
        "        # 예측 수행\n",
        "        preds = predict(model, test_loader, device)\n",
        "        ensemble_preds.append(preds)\n",
        "\n",
        "    # 모든 모델의 예측값 평균\n",
        "    final_preds = np.mean(ensemble_preds, axis=0)\n",
        "    return final_preds\n",
        "\n",
        "# 설정\n",
        "model_paths = [\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\best_model_fold_0.pth\",\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\best_model_fold_1.pth\",\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\final_best_model.pth\"\n",
        "]\n",
        "model_name = \"deit3_small_patch16_224.fb_in22k_ft_in1k\"  # 학습에 사용한 모델 이름\n",
        "test_csv_path = r\"C:\\wockd\\ultra\\ultra\\retina\\test.csv\"\n",
        "output_file = r\"C:\\wockd\\ultra\\ultra\\retina\\final_predictions.csv\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# 테스트 데이터 로드\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# 데이터셋 생성\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_paths, labels=None, transforms=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.labels = labels  # labels가 필요하다면 추가\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        from PIL import Image\n",
        "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img\n",
        "\n",
        "# 이미지 변환 설정 (필요에 따라 수정)\n",
        "from torchvision import transforms\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# 테스트 데이터셋 및 DataLoader\n",
        "test_dataset = CustomDataset(\n",
        "    img_paths=test_df['img_path'].values,\n",
        "    transforms=test_transform\n",
        ")\n",
        "\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "# 앙상블 수행\n",
        "final_predictions = ensemble_predictions(model_paths, model_name, test_loader, device)\n",
        "\n",
        "# 최종 클래스 결정\n",
        "final_labels = np.argmax(final_predictions, axis=1)\n",
        "\n",
        "# 결과 저장\n",
        "test_df['label'] = final_labels\n",
        "test_df[['id', 'label']].to_csv(output_file, index=False)\n",
        "print(f\"앙상블 결과가 {output_file}에 저장되었습니다.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gThQGhzjQMNn",
        "outputId": "94498189-f379-40c8-93ff-3b0d4b35d65b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\wockd\\AppData\\Local\\Temp\\ipykernel_2368\\2719723625.py:31: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  state_dict = torch.load(model_path, map_location=device)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Label mapping keys: dict_keys([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25])\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Predicting: 100%|██████████| 107/107 [00:14<00:00,  7.16it/s]\n",
            "Predicting: 100%|██████████| 107/107 [00:14<00:00,  7.51it/s]\n",
            "Predicting: 100%|██████████| 107/107 [00:14<00:00,  7.26it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Missing mappings for labels [571, 51, 353, 51, 34, 51, 51, 353, 571, 353, 51, 51, 353, 34, 353, 51, 34, 34, 51, 34, 34, 34, 34, 34, 51, 51, 34, 34, 34, 51, 353, 353, 353, 353, 34, 51, 350, 353, 353, 51, 51, 34, 34, 51, 51, 51, 34, 353, 34, 51, 51, 353, 353, 276, 571, 353, 353, 51, 34, 353, 51, 51, 353, 34, 34, 353, 353, 34, 34, 34, 34, 51, 51, 353, 34, 353, 34, 353, 34, 51, 353, 34, 51, 353, 353, 353, 353, 51, 353, 353, 51, 353, 353, 353, 51, 51, 353, 51, 51, 34, 353, 51, 51, 34, 51, 353, 34, 509, 276, 34, 51, 51, 34, 353, 34, 51, 353, 51, 34, 571, 797, 353, 34, 34, 51, 34, 51, 353, 51, 51, 51, 51, 353, 51, 51, 34, 51, 51, 51, 34, 353, 353, 353, 51, 797, 353, 51, 51, 34, 34, 51, 34, 353, 34, 353, 353, 51, 353, 34, 51, 51, 51, 51, 34, 353, 34, 51, 51, 34, 51, 353, 34, 353, 51, 353, 51, 34, 51, 353, 51, 353, 34, 353, 51, 353, 353, 34, 51, 353, 797, 34, 51, 353, 34, 51, 51, 571, 353, 353, 51, 509, 51, 34, 353, 353, 34, 34, 34, 353, 353, 51, 353, 51, 51, 353, 34, 797, 51, 353, 276, 353, 34, 353, 34, 51, 34, 353, 34, 353, 51, 571, 571, 353, 353, 34, 34, 34, 51, 34, 353, 51, 353, 276, 51, 51, 353, 353, 353, 34, 34, 353, 34, 51, 571, 34, 353, 353, 51, 353, 353, 353, 51, 51, 353, 34, 51, 34, 353, 51, 353, 34, 51, 34, 51, 51, 353, 51, 353, 51, 34, 34, 51, 34, 350, 353, 51, 353, 51, 353, 353, 51, 51, 51, 353, 34, 34, 34, 353, 34, 51, 34, 571, 34, 353, 353, 34, 51, 51, 34, 353, 276, 353, 34, 34, 51, 51, 34, 353, 353, 51, 51, 51, 51, 51, 353, 353, 509, 34, 353, 353, 353, 34, 34, 34, 353, 51, 51, 51, 571, 34, 34, 51, 51, 51, 51, 353, 353, 51, 34, 34, 51, 353, 34, 34, 51, 51, 34, 34, 34, 353, 51, 353, 353, 353, 353, 34, 353, 353, 353, 34, 51, 34, 34, 520, 353, 353, 51, 353, 353, 34, 34, 353, 353, 51, 34, 51, 51, 353, 353, 51, 51, 34, 353, 51, 51, 509, 51, 353, 509, 797, 51, 353, 571, 34, 353, 34, 353, 353, 353, 353, 353, 34, 51, 51, 571, 353, 353, 571, 353, 51, 51, 353, 51, 353, 350, 51, 34, 353, 353, 51, 51, 353, 51, 34, 797, 353, 34, 353, 34, 51, 51, 353, 353, 34, 51, 34, 353, 353, 34, 51, 353, 571, 353, 353, 51, 34, 34, 353, 34, 34, 34, 51, 276, 353, 353, 34, 34, 34, 353, 797, 34, 51, 353, 571, 571, 51, 34, 51, 34, 276, 51, 34, 51, 34, 34, 353, 34, 34, 353, 353, 51, 51, 353, 51, 353, 276, 353, 51, 353, 353, 353, 51, 353, 51, 34, 34, 34, 353, 34, 353, 51, 51, 353, 51, 51, 353, 353, 34, 51, 34, 353, 34, 34, 34, 797, 51, 34, 34, 353, 353, 51, 509, 51, 34, 34, 276, 353, 51, 51, 51, 51, 353, 34, 353, 353, 353, 51, 51, 353, 34, 51, 34, 353, 51, 34, 51, 34, 34, 51, 353, 51, 51, 34, 34, 353, 51, 34, 34, 34, 571, 353, 34, 51, 34, 51, 51, 34, 51, 51, 51, 34, 353, 51, 353, 34, 571, 51, 353, 353, 353, 797, 34, 34, 353, 34, 34, 51, 353, 353, 34, 51, 51, 51, 353, 353, 353, 51, 353, 276, 353, 353, 353, 353, 353, 51, 51, 51, 34, 51, 34, 51, 51, 51, 34, 353, 353, 353, 34, 51, 797, 353, 51, 51, 34, 34, 51, 34, 34, 34, 51, 353, 51, 34, 276, 353, 51, 34, 51, 353, 34, 34, 353, 34, 353, 276, 51, 51, 34, 350, 276, 353, 51, 353, 353, 34, 520, 797, 353, 51, 51, 353, 353, 34, 353, 353, 51, 34, 34, 34, 353, 353, 276, 353, 34, 353, 51, 34, 51, 353, 51, 353, 353, 51, 353, 353, 797, 34, 34, 34, 51, 34, 51, 51, 51, 51, 353, 797, 51, 34, 350, 353, 51, 353, 353, 51, 51, 51, 34, 51, 34, 34, 353, 34, 51, 51, 34, 51, 797, 353, 34, 353, 51, 353, 51, 51, 353, 353, 51, 34, 51, 353, 51, 51, 353, 34, 34, 51, 509, 353, 509, 353, 797, 34, 51, 51, 353, 34, 353, 51, 353, 353, 353, 353, 797, 51, 353, 51, 353, 34, 353, 51, 34, 353, 353, 34, 34, 51, 797, 51, 353, 51, 509, 34, 353, 353, 34, 353, 353, 34, 353, 350, 34, 353, 51, 51, 51, 353, 353, 51, 51, 51, 353, 51, 34, 353, 797, 353, 353, 51, 51, 797, 34, 353, 353, 51, 51, 353, 34, 34, 353, 51, 353, 34, 34, 51, 353, 353, 34, 51, 34, 51, 34, 353, 353, 353, 353, 353, 51, 353, 34, 353, 353, 797, 34, 34, 34, 51, 797, 51, 276, 51, 353, 353, 34, 797, 353, 353, 353, 34, 51, 34, 34, 34, 276, 353, 276, 571, 353, 276, 34, 51, 51, 797, 353, 51, 34, 51, 51, 34, 51, 34, 51, 353, 34, 353, 353, 34, 353, 276, 353, 34, 34, 34, 51, 51, 51, 34, 34, 34, 353, 353, 34, 51, 34, 34, 34, 353, 51, 34, 353, 353, 34, 353, 34, 34, 51, 571, 51, 34, 353, 51, 34, 34, 51, 797, 51, 51, 353, 51, 51, 34, 51, 353, 51, 353, 353, 353, 51, 353, 51, 34, 353, 353, 353, 34, 51, 353, 34, 353, 51, 34, 51, 34, 34, 353, 51, 51, 353, 353, 51, 34, 353, 51, 353, 34, 353, 353, 350, 353, 51, 797, 51, 276, 34, 51, 34, 34, 353, 51, 797, 353, 51, 51, 51, 51, 51, 353, 353, 353, 34, 51, 51, 353, 51, 51, 353, 51, 353, 51, 34, 34, 51, 51, 34, 353, 34, 51, 34, 51, 353, 51, 34, 34, 34, 51, 51, 51, 51, 353, 51, 51, 353, 34, 51, 51, 51, 353, 353, 51, 276, 51, 51, 34, 51, 353, 51, 51, 353, 276, 353, 353, 34, 353, 353, 51, 34, 51, 353, 51, 34, 353, 353, 51, 34, 51, 34, 353, 353, 353, 51, 51, 353, 34, 51, 353, 353, 353, 353, 353, 353, 34, 350, 51, 353, 34, 353, 51, 34, 51, 353, 34, 353, 51, 34, 353, 353, 797, 276, 34, 353, 51, 51, 353, 353, 51, 353, 353, 353, 51, 353, 353, 51, 353, 353, 353, 51, 34, 353, 353, 51, 51, 51, 353, 34, 34, 353, 353, 51, 353, 51, 51, 353, 51, 34, 51, 353, 51, 353, 276, 353, 34, 353, 353, 276, 353, 34, 51, 51, 797, 353, 51, 34, 34, 276, 51, 350, 509, 353, 276, 353, 34, 353, 51, 51, 353, 353, 353, 51, 51, 34, 353, 353, 34, 571, 34, 34, 353, 353, 34, 51, 34, 276, 51, 353, 34, 276, 51, 353, 353, 51, 353, 353, 353, 34, 51, 51, 34, 51, 353, 353, 34, 353, 520, 353, 34, 353, 353, 353, 34, 34, 276, 51, 34, 353, 51, 353, 51, 34, 34, 34, 353, 51, 34, 51, 276, 353, 353, 353, 353, 353, 51, 51, 34, 353, 34, 353, 51, 51, 571, 51, 353, 353, 34, 51, 353, 51, 353, 353, 353, 353, 51, 34, 353, 353, 51, 51, 34, 51, 353, 353, 353, 34, 34, 353, 353, 276, 353, 51, 276, 34, 353, 51, 353, 353, 353, 34, 276, 34, 353, 353, 353, 353, 51, 51, 51, 571, 353, 353, 51, 353, 51, 51, 353, 51, 34, 51, 51, 353, 34, 353, 353, 34, 51, 34, 353, 51, 353, 51, 51, 353, 51, 51, 51, 797, 353, 353, 51, 797, 353, 353, 353, 34, 51, 51, 797, 51, 34, 34, 353, 353, 353, 571, 797, 51, 34, 353, 797, 51, 51, 34, 34, 353, 51, 34, 353, 51, 51, 34, 353, 353, 353, 571, 34, 34, 571, 353, 51, 51, 353, 34, 34, 797, 51, 353, 51, 353, 350, 51, 353, 353, 353, 34, 353, 353, 34, 276, 34, 353, 797, 34, 353, 51, 353, 353, 34, 34, 34, 51, 571, 51, 51, 353, 34, 276, 34, 51, 34, 353, 353, 34, 353, 353, 51, 51, 51, 353, 51, 353, 353, 34, 34, 353, 571, 353, 353, 34, 353, 571, 353, 34, 571, 51, 353, 34, 34, 51, 51, 51, 34, 353, 353, 51, 353, 51, 353, 353, 51, 353, 353, 353, 51, 34, 353, 353, 51, 276, 34, 34, 34, 353, 353, 51, 51, 353, 34, 34, 353, 353, 353, 797, 276, 51, 51, 797, 34, 34, 34, 51, 51, 34, 34, 353, 353, 353, 353, 51, 51, 571, 353, 51, 34, 353, 353, 34, 509, 51, 34, 34, 51, 353, 353, 353, 34, 34, 51, 51, 353, 51, 353, 571, 51, 353, 353, 34, 34, 353, 353, 353, 34, 353, 353, 34, 353, 34, 34, 51, 34, 353, 34, 571, 34, 353, 51, 51, 34, 34, 797, 34, 34, 276, 353, 797, 34, 51, 353, 353, 34, 353, 51, 34, 51, 34, 353, 34, 34, 276, 353, 51, 353, 353, 353, 797, 34, 51, 34, 51, 353, 34, 276, 51, 51, 51, 51, 34, 353, 353, 571, 797, 34, 34, 34, 51, 571, 51, 353, 276, 51, 350, 353, 353, 353, 34, 34, 34, 353, 34, 353, 353, 51, 51, 353, 797, 34, 34, 51, 353, 571, 51, 276, 353, 353, 353, 51, 353, 34, 276, 51, 51, 353, 571, 353, 797, 34, 34, 34, 34, 353, 350, 51, 353, 51, 34, 353, 34, 353, 353, 34, 353, 51, 51, 51, 353, 353, 353, 34, 51, 353, 51, 51, 51, 353, 353, 51, 34, 353, 51, 34, 51, 51, 34, 51, 51, 51, 34, 34, 51, 34, 353, 34, 353, 51, 51, 353, 34, 51, 353, 571, 51, 51, 353, 797, 276, 34, 34, 51, 353, 353, 51, 353, 34, 51, 51, 51, 34, 51, 34, 353, 51, 34, 276, 34, 353, 51, 34, 51, 34, 51, 353, 34, 34, 353, 353, 51, 51, 34, 34, 353, 51, 51, 51, 353, 353, 51, 51, 353, 34, 353, 350, 34, 51, 34, 51, 34, 34, 34, 51, 51, 51, 353, 34, 51, 51, 807, 34, 353, 34, 51, 571, 51, 51, 353, 34, 353, 353, 51, 34, 51, 34, 353, 353, 51, 51, 34, 276, 51, 34, 353, 51, 353, 353, 350, 34, 353, 353, 509, 34, 353, 353, 571, 276, 51, 34, 34, 353, 353, 51, 34, 353, 353, 34, 34, 51, 34, 353, 34, 353, 51, 51, 350, 276, 51, 353, 353, 353, 51, 51, 353, 353, 276, 51, 51, 571, 797, 51, 51, 276, 34, 51, 51, 353, 51, 276, 353, 51, 353, 34, 353, 51, 353, 353, 353, 34, 353, 353, 520, 51, 353, 51, 353, 51, 353, 51, 51, 353, 51, 34, 51, 51, 34, 353, 34, 353, 353, 353, 571, 34, 353, 51, 353, 509, 353, 353, 34, 51, 353, 807, 34, 353, 34, 34, 51, 51, 34, 276, 34, 353, 571, 51, 353, 34, 34, 34, 353, 34, 51, 353, 34, 34, 353, 276, 353, 353, 353, 353, 51, 51, 34, 51, 353, 34, 34, 353, 34, 51, 51, 51, 353, 797, 51, 353, 353, 353, 34, 353, 51, 51, 353, 51, 51, 353, 51, 353, 51, 51, 51, 34, 34, 353, 34, 51, 353, 353, 51, 353, 276, 51, 51, 353, 51, 353, 51, 34, 34, 34, 353, 51, 276, 509, 51, 34, 34, 353, 353, 51, 34, 34, 353, 353, 353, 51, 51, 797, 51, 34, 353, 34, 353, 797, 34, 353, 353, 34, 276, 51, 571, 34, 353, 571, 51, 353, 353, 34, 34, 353, 51, 353, 353, 353, 51, 34, 51, 353, 353, 34, 353, 51, 353, 353, 353, 797, 34, 51, 51, 353, 353, 51, 34, 353, 353, 51, 353, 353, 34, 51, 51, 353, 353, 353, 51, 51, 51, 51, 34, 353, 34, 353, 51, 34, 807, 353, 51, 353, 34, 34, 353, 353, 51, 353, 51, 353, 34, 34, 51, 353, 51, 34, 51, 34, 51, 51, 34, 353, 353, 34, 51, 353, 51, 51, 51, 353, 51, 34, 34, 34, 51, 51, 34, 34, 34, 509, 34, 51, 51, 353, 353, 51, 34, 353, 51, 34, 353, 51, 353, 34, 353, 571, 509, 51, 353, 34, 51, 34, 353, 353, 34, 353, 51, 353, 353, 34, 350, 51, 353, 51, 51, 51, 34, 51, 353, 51, 350, 34, 51, 571, 797, 353, 34, 353, 353, 34, 34, 51, 34, 34, 51, 51, 34, 353, 34, 34, 353, 51, 34, 353, 51, 353, 34, 353, 571, 571, 34, 51, 34, 34, 353, 34, 51, 51, 353, 34, 51, 51, 353, 353, 51, 353, 51, 51, 51, 51, 51, 34, 51, 571, 51, 34, 34, 353, 51, 353, 34, 34, 34, 34, 51, 34, 34, 797, 51, 51, 51, 51, 51, 51, 353, 34, 51, 34, 34, 353, 353, 34, 34, 34, 51, 34, 34, 51, 276, 353, 797, 353, 353, 34, 51, 51, 353, 51, 34, 276, 276, 51, 350, 51, 353, 353, 34, 34, 51, 51, 34, 509, 353, 51, 34, 34, 353, 34, 34, 51, 51, 51, 34, 571, 51, 34, 276, 353, 34, 51, 34, 34, 34, 353, 34, 353, 34, 51, 353, 51, 34, 353, 34, 34, 51, 51, 34, 353, 353, 34, 51, 353, 353, 353, 51, 276, 51, 51, 353, 34, 353, 51, 51, 34, 34, 276, 51, 797, 353, 353, 353, 51, 276, 34, 509, 51, 51, 51, 34, 51, 34, 34, 353, 353, 51, 34, 353, 509, 51, 571, 51, 34, 51, 51, 34, 571, 34, 353, 353, 276, 34, 276, 353, 353, 51, 353, 34, 276, 353, 34, 571, 34, 34, 51, 51, 353, 34, 353, 353, 353, 51, 34, 51, 353, 276, 353, 51, 34, 34, 353, 51, 34, 51, 353, 353, 34, 353, 353, 353, 34, 51, 353, 51, 353, 797, 509, 51, 353, 34, 34, 51, 350, 51, 353, 350, 353, 353, 34, 353, 51, 276, 34, 353, 353, 51, 51, 51, 51, 51, 353, 51, 571, 353, 51, 276, 797, 353, 353, 34, 34, 34, 34, 51, 276, 51, 353, 353, 34, 51, 51, 34, 353, 51, 34, 34, 276, 34, 353, 353, 34, 51, 34, 276, 51, 34, 34, 353, 353, 51, 353, 350, 353, 34, 353, 353, 353, 353, 509, 353, 353, 353, 353, 276, 353, 276, 34, 353, 34, 51, 51, 51, 353, 353, 353, 353, 51, 350, 34, 51, 34, 353, 34, 353, 51, 353, 51, 34, 509, 51, 34, 34, 51, 34, 51, 353, 353, 353, 353, 276, 571, 353, 353, 51, 34, 353, 51, 34, 353, 34, 51, 34, 276, 34, 353, 34, 353, 51, 353, 353, 51, 353, 353, 350, 51, 353, 353, 34, 350, 353, 51, 34, 51, 353, 34, 276, 51, 353, 353, 34, 353, 353, 353, 34, 34, 51, 51, 353, 34, 34, 34, 353, 51, 51, 34, 353, 51, 34, 51, 353, 51, 509, 51, 353, 34, 34, 353, 51, 34, 51, 353, 34, 353, 34, 51, 34, 353, 353, 51, 51, 353, 353, 353, 34, 797, 34, 353, 350, 34, 34, 353, 353, 34, 34, 34, 51, 51, 51, 34, 51, 51, 51, 276, 51, 34, 276, 51, 353, 34, 34, 34, 51, 51, 51, 51, 51, 353, 34, 51, 350, 34, 353, 353, 276, 34, 51, 797, 51, 51, 51, 353, 353, 353, 51, 51, 353, 353, 353, 51, 51, 353, 34, 353, 353, 34, 353, 51, 51, 51, 353, 797, 51, 350, 51, 571, 34, 353, 509, 51, 51, 353, 350, 353, 353, 353, 276, 797, 34, 34, 34, 51, 797, 353, 51, 353, 353, 34, 353, 276, 353, 276, 353, 51, 353, 51, 797, 51, 34, 51, 51, 276, 51, 51, 34, 34, 51, 51, 34, 34, 34, 353, 34, 51, 797, 34, 34, 34, 571, 34, 51, 51, 51, 34, 353, 353, 353, 797, 353, 34, 353, 807, 353, 353, 34, 51, 34, 353, 34, 51, 34, 353, 51, 51, 34, 34, 34, 353, 353, 51, 34, 353, 353, 350, 34, 353, 353, 34, 353, 797, 51, 51, 353, 51, 353, 34, 34, 51, 353, 51, 571, 34, 51, 34, 353, 797, 353, 51, 807, 353, 51, 353, 34, 353, 51, 34, 34, 353, 276, 34, 51, 276, 353, 34, 34, 51, 51, 51, 353, 353, 51, 353, 51, 353, 34, 51, 353, 34, 353, 353, 51, 353, 353, 353, 353, 353, 353, 34, 353, 34, 51, 51, 353, 51, 34, 509, 276, 34, 51, 571, 571, 34, 51, 51, 51, 51, 51, 353, 353, 353, 34, 51, 34, 51, 353, 51, 353, 51, 51, 353, 276, 353, 51, 34, 353, 51, 571, 34, 571, 51, 353, 353, 51, 34, 571, 34, 51, 353, 51, 353, 51, 51, 51, 34, 34, 51, 353, 51, 51, 353, 34, 353, 353, 353, 51, 34, 34, 34, 797, 51, 353, 51, 51, 34, 34, 34, 353, 51, 51, 353, 34, 34, 51, 34, 51, 51, 51, 353, 51, 51, 353, 51, 51, 353, 353, 34, 51, 51, 353, 51, 353, 51, 353, 353, 34, 51, 353, 353, 353, 51, 571, 353, 353, 353, 51, 34, 276, 51, 34, 34, 353, 797, 51, 51, 571, 350, 34, 571, 34, 353, 51, 34, 353, 34, 353, 34, 51, 353, 353, 51, 34, 353, 34, 34, 353, 571, 353, 34, 797, 51, 353, 51, 51, 350, 353, 34, 51, 51, 353, 34, 353, 51, 34, 51, 51, 34, 51, 353, 34, 353, 353, 51, 51, 353, 51, 353, 51, 34, 51, 34, 353, 797, 34, 353, 51, 34, 353, 797, 353, 353, 353, 571, 353, 353, 51, 571, 34, 353, 51, 353, 353, 51, 34, 34, 34, 353, 353, 34, 353, 353, 353, 51, 353, 353, 353, 51, 353, 51, 34, 353, 571, 34, 51, 353, 350, 51, 571, 51, 276, 51, 509, 353, 353, 51, 51, 34, 34, 34, 353, 353, 34, 353, 353, 34, 34, 51, 51, 51, 51, 51, 51, 353, 51, 51, 353, 353, 51, 51, 51, 51, 34, 51, 51, 353, 353, 34, 34, 353, 797, 353, 51, 353, 353, 34, 34, 353, 34, 51, 520, 51, 353, 51, 353, 34, 51, 353, 353, 571, 353, 34, 353, 353, 51, 34, 353, 353, 34, 51, 797, 34, 353, 34, 34, 34, 34, 34, 353, 353, 51, 34, 34, 34, 34, 51, 51, 51, 353, 51, 353, 34, 509, 350, 353, 51, 51, 353, 353, 51, 34, 51, 51, 276, 353, 353, 353, 51, 51, 51, 51, 353, 34, 353, 51, 34, 51, 51, 51, 353, 353, 353, 34, 276, 353, 353, 353, 34, 34, 34, 51, 34, 353, 353, 51, 353, 353, 353, 34, 509, 276, 51, 34, 51, 34, 34, 51, 51, 51, 353, 276, 51, 51, 51, 34, 353, 34, 353, 797, 51, 571, 51, 51, 353, 353, 353, 353, 353, 353, 51, 276, 353, 34, 353, 353, 797, 353, 353, 353, 353, 51, 353, 34, 34, 34, 51, 51, 353, 353, 34, 34, 34, 350, 34, 276, 34, 34, 51, 51, 51, 34, 353, 353, 34, 34, 353, 353, 353, 51, 51, 276, 51, 34, 276, 51, 51, 353, 51, 353, 34, 353, 34, 51, 34, 353, 353, 51, 34, 51, 34, 353, 51, 34, 51, 34, 34, 34, 51, 34, 353, 34, 509, 51, 51, 34, 51, 51, 34, 51, 34, 34, 353, 350, 353, 797, 51, 51, 353, 51, 34, 51, 51, 51, 34, 51, 51, 34, 51, 51, 51, 51, 34, 34, 34, 353, 353, 353, 571, 34, 353, 51, 51, 51, 51, 353, 353, 51, 353, 34, 353, 34, 34, 34, 34, 34, 51, 51, 797, 51, 51, 353, 353, 353, 51, 51, 34, 34, 34, 34, 34, 51, 353, 353, 353, 51, 353, 34, 353, 51, 34, 353, 34, 353, 51, 34, 51, 34, 51, 571, 571, 51, 51, 520, 353, 51, 51, 34, 51, 34, 353, 353, 51, 51, 34, 51, 51, 353, 353, 51, 353, 51, 353, 34, 353, 353, 34, 51, 34, 353, 51, 34, 34, 353, 353, 353, 51, 353, 34, 353, 51, 34, 276, 353, 969, 353, 353, 353, 34, 353, 353, 353, 353, 34, 51, 51, 51, 353, 276, 34, 51, 34, 353, 34, 51, 51, 353, 353, 353, 51, 51, 51, 353, 353, 34, 34, 34, 51, 34, 51, 34, 34, 353, 34, 797, 353, 51, 34, 353, 34, 353, 51, 51, 34, 34, 276, 51, 353, 797, 353, 353, 353, 353, 34, 276, 34, 353, 509, 276, 34, 34, 34, 353, 353, 34, 34, 34, 34, 51, 51, 34, 51, 51, 51, 353, 353, 34, 51, 34, 353, 51, 353, 51, 353, 34, 51, 34, 797, 276, 51, 51, 353, 34, 353, 797, 34, 276, 51, 34, 34, 51, 51, 51, 353, 34, 51, 797, 34, 34, 51, 51, 51, 51, 276, 353, 353, 51, 51, 353, 51, 353, 353, 51, 353, 353, 353, 34, 353, 51, 51, 353, 34, 353, 353, 571, 34, 353, 353, 34, 353, 34, 353, 353, 51, 34, 34, 353, 34, 797, 34, 353, 34, 51, 34, 34, 353, 51, 353, 350, 276, 51, 34, 353, 353, 51, 51, 51, 353, 34, 353, 571, 51, 51, 51, 34, 34, 34, 353, 350, 34, 353, 51, 353, 353, 34, 51, 353, 51, 34, 353, 797, 51, 51, 51, 797, 51, 353, 571, 51, 276, 51, 51, 353, 353, 51, 51, 353, 34, 51, 34, 34, 51, 353, 51, 34, 353, 353, 51, 797, 34, 51, 353, 353, 51, 353, 353, 51, 34, 353, 353, 34, 51, 353, 51, 353, 51, 51, 571, 34, 353, 34, 51, 353, 353, 51, 51, 51, 353, 353, 571, 797, 51, 51, 51, 51, 51, 353, 51, 34, 51, 353, 353, 51, 353, 353, 34, 34, 34, 34, 51, 51, 353, 34, 353, 353, 571, 353, 353, 51, 353, 276, 353, 34, 34, 34, 34, 34, 51, 51, 34, 51, 34, 34, 353, 34, 797, 51, 51, 34, 353, 353, 51, 51, 51, 51, 34, 51, 353, 353, 353, 34, 571, 34, 34, 350, 353, 353, 34, 51, 34, 34, 34, 353, 353, 51, 353, 353, 34, 353, 34, 51, 353, 34, 34, 51, 353, 34, 353, 797, 353, 353, 51, 353, 353, 353, 353, 51, 34, 51, 34, 51, 353, 51, 353, 353, 353, 51, 34, 34, 51, 34, 34, 353, 51, 34, 353, 51, 51, 353, 51, 353, 51, 353, 51, 353, 34, 276, 353, 34, 51, 350, 34, 571, 797, 51, 34, 571, 353, 353, 34, 34, 353, 353, 353, 350, 51, 51, 51, 51, 51, 353, 797, 51, 51, 571, 353, 34, 34, 797, 34, 353, 797, 51, 353, 571, 34, 51, 34, 353, 353, 34, 797, 34, 276, 51, 51, 51, 353, 34, 353, 34, 34, 51, 571, 797, 51, 353, 276, 34, 353, 34, 353, 353, 51, 353, 353, 51, 34, 353, 34, 353, 353, 353, 353, 34, 34, 51, 34, 353, 276, 51, 34, 51, 350, 353, 34, 51, 34, 51, 353, 353, 350, 51, 353, 51, 353, 353, 353, 51, 51, 353, 797, 353, 350, 51, 51, 353, 353, 51, 353, 34, 51, 51, 51, 571, 797, 353, 34, 51, 34, 34, 353, 34, 34, 34, 353, 51, 51, 350, 353, 353, 353, 34, 51, 353, 353, 571, 51, 51, 51, 34, 353, 350, 34, 353, 51, 51, 34, 34, 797, 353, 353, 51, 276, 51, 51, 353, 51, 34, 353, 353, 34, 353, 34, 34, 353, 353, 51, 34, 353, 34, 51, 51, 509, 51, 51, 353, 353, 34, 797, 797, 34, 353, 353, 353, 51, 51, 353, 51, 353, 34, 34, 51, 353, 353, 276, 353, 51, 353, 51, 353, 51, 34, 51, 34, 51, 571, 353, 51, 51, 353, 51, 51, 34, 353, 350, 353, 51, 51, 353, 34, 51, 353, 353, 34, 51, 353, 34, 51, 34, 353, 350, 353, 353, 353, 34, 34, 353, 797, 34, 353, 353, 34, 34, 353, 353, 51, 353, 353, 276, 353, 34, 353, 353, 34, 353, 34, 51, 353, 51, 34, 353, 353, 51, 34, 51, 34, 51, 51, 34, 276, 34, 353, 34, 353, 51, 353, 797, 353, 51, 353, 51, 34, 353, 51, 51, 34, 353, 34, 353, 51, 34, 34, 34, 353, 51, 51, 51, 34, 51, 34, 571, 353, 51, 34, 51, 34, 51, 51, 51, 51, 571, 51, 353, 51, 51, 509, 353, 51, 34, 353, 51, 353, 353, 51, 51, 571, 51, 353, 353, 51, 353, 51, 51, 34, 51, 51, 34, 51, 34, 353, 51, 34, 34, 51, 34, 353, 353, 353, 51, 353, 51, 276, 34, 34, 51, 350, 353, 51, 51, 34, 34, 51, 353, 34, 571, 353, 34, 276, 353, 34, 509, 34, 353, 34, 34, 353, 51, 571, 276, 353, 51, 353, 51, 353, 34, 353, 353, 353, 51, 51, 353, 34, 353, 276, 34, 276, 353, 350, 353, 51, 51, 51, 353, 51, 34, 34, 34, 51, 571, 353, 51, 51, 34, 353, 51, 350, 353, 353, 353, 34, 34, 797, 51, 34, 353, 51, 51, 353, 51, 51, 571, 353, 51, 353, 353, 34, 353, 51, 51, 353, 51, 51, 34, 969, 353, 353, 353, 276, 34, 34, 51, 34, 353, 353, 34, 51, 353, 51, 51, 34, 353, 353, 353, 51, 34, 353, 51, 34, 353, 353, 51, 353, 353, 353, 34, 51, 353, 51, 353, 34, 34, 353, 51, 34, 34, 34, 34, 353, 353, 353, 353, 797, 51, 353, 353, 797, 509, 353, 353, 353, 353, 797, 353, 51, 276, 353, 51, 34, 51, 51, 797, 51, 353, 353, 34, 509, 51, 353, 34, 51, 353, 51, 509, 353, 51, 571, 51, 34, 34, 353, 353, 353, 51, 34, 353, 353, 353, 353, 276, 51, 353, 509, 51, 51, 34, 34, 34, 353, 571, 353, 353, 51, 353, 353, 353, 353, 797, 571, 797, 51, 34, 34, 34, 34, 353, 353, 350, 353, 353, 34, 34, 51, 353, 353, 276, 34, 34, 51, 276, 353, 51, 34, 51, 34, 353, 51, 353, 51, 353, 353, 34, 353, 34, 353, 51, 34, 276, 34, 353, 353, 34, 51, 51, 51, 353, 34, 51, 34, 353, 353, 34, 276, 353, 353, 51, 51, 51, 353, 353, 34, 51, 353, 353, 353, 51, 51, 353, 353, 509, 34, 51, 51, 276, 353, 34, 51, 51, 34, 509, 34, 34, 797, 51, 353, 353, 353, 353, 353, 353, 51, 51, 51, 353, 276, 51, 51, 353, 34, 353, 353, 353, 51, 34, 797, 51, 34, 34, 34, 34, 34, 353, 353, 353, 571, 51, 51, 34, 353, 353, 34, 353, 34, 51, 353, 353, 353, 51, 353, 353, 34, 571, 34, 353, 34, 34, 34, 34, 353, 51, 353, 51, 34, 34, 51, 353, 34, 353, 51, 34, 353, 51, 34, 350, 276, 51, 34, 51, 51, 353, 51, 353, 51, 51, 353, 51, 34, 353, 34, 353, 34, 276, 34, 353, 51, 353, 353, 353, 797, 34, 51, 51, 34, 51, 353, 353, 353, 34, 51, 353, 353, 353, 34, 353, 51, 34, 353, 353, 51, 353, 51, 51, 51, 353, 353, 51, 51, 34, 34, 51, 51, 51, 51, 34, 353, 353, 353, 34, 571, 353, 34, 34, 350, 353, 353, 51, 51, 51, 51, 353, 797, 51, 34, 34, 51, 353, 353, 353, 51, 353, 571, 276, 51, 51, 34, 51, 353, 51, 34, 509, 353, 34, 34, 34, 34, 51, 51, 51, 353, 34, 34, 353, 34, 51, 51, 34, 51, 276, 353, 51, 353, 353, 353, 353, 51, 51, 34, 353, 34, 509, 51, 353, 34, 51, 34, 797, 51, 353, 34, 51, 51, 51, 34, 571, 353, 34, 353, 797, 797, 353, 353, 51, 353, 51, 353, 51, 571, 51, 34, 353, 34, 51, 34, 51, 571, 51, 51, 34, 34, 353, 353, 34, 571, 51, 34, 34, 353, 34, 353, 353, 509, 34, 34, 353, 353, 34, 276, 34, 34, 51, 34, 51, 51, 353, 34, 353, 51, 353, 51, 51, 51, 51, 34, 51, 353, 51, 51, 51, 353, 34, 353, 353, 34, 34, 34, 34, 34, 51, 353, 353, 51, 51, 353, 51, 34, 51, 51, 51, 353, 34, 51, 51, 353, 34, 353, 353, 353, 353, 34, 51, 353, 51, 353, 353, 353, 51, 34, 353, 34, 353, 51, 353, 51, 51, 353, 353, 34, 353, 353, 353, 34, 353, 509, 34, 51, 51, 350, 51, 353, 51, 51, 34, 353, 34, 353, 51, 51, 353, 51, 34, 353, 353, 353, 34, 51, 353, 34, 51, 51, 51, 353, 34, 51, 51, 353, 34, 51, 51, 51, 353, 34, 353, 353, 353, 34, 34, 571, 51, 276, 353, 51, 353, 34, 34, 353, 51, 51, 51, 34, 353, 51, 34, 51, 51, 353, 353, 34, 34, 34, 51, 34, 353, 34, 353, 571, 34, 509, 353, 509, 797, 51, 51, 51, 353, 34, 353, 353, 353, 51, 353, 51, 51, 353, 51, 353, 34, 34, 353, 51, 34, 353, 34, 51, 34, 353, 34, 34, 51, 51, 353, 34, 51, 51, 51, 51, 51, 51, 353, 353, 51, 51, 34, 353, 353, 353, 51, 353, 51, 34, 51, 34, 51, 51, 353, 51, 34, 353, 34, 571, 51, 51, 34, 34, 353, 34, 353, 353, 350, 353, 353, 51, 34, 34, 353, 51, 353, 34, 51, 571, 34, 353, 797, 51, 797, 353, 353, 34, 34, 51, 34, 34, 353, 353, 51, 51, 571, 51, 51, 34, 353, 34, 353, 353, 276, 353, 51, 797, 353, 51, 353, 353, 509, 34, 34, 353, 276, 571, 353, 353, 51, 51, 353, 51, 34, 34, 353, 51, 34, 51, 51, 51, 353, 34, 353, 51, 353, 353, 353, 353, 51, 353, 353, 51, 51, 353, 34, 353, 51, 51, 34, 51, 353, 51, 51, 34, 34, 51, 353, 353, 353, 34, 350, 51, 51, 51, 353, 350, 51, 353, 34, 353, 51, 353, 353, 353, 353, 34, 276, 51, 353, 51, 353, 353, 353, 34, 34, 51, 34, 51, 51, 34, 51, 353, 34, 34, 350, 51, 353, 51, 51, 276, 51, 51, 276, 353, 51, 353, 51, 34, 353, 34, 353, 34, 276, 353, 353, 51, 353, 350, 51, 353, 353, 51, 51, 34, 797, 34, 353, 34, 51, 51, 353, 353, 353, 34, 51, 353, 353, 797, 51, 797, 51, 353, 350, 797, 353, 353, 34, 353, 34, 353, 51, 51, 797, 353, 51, 51, 353, 353, 353, 353, 51, 353, 34, 51, 353, 51, 353, 353, 797, 34, 51, 51, 34, 34, 797, 353, 34, 353, 51, 34, 51, 797, 34, 51, 353, 353, 353, 34, 51, 51, 353, 353, 51, 34, 34, 797, 51, 51, 51, 51, 51, 34, 34, 51, 34, 353, 353, 34, 51, 353, 353, 51, 34, 34, 34, 51, 571, 51, 353, 353, 353, 571, 51, 353, 34, 51, 571, 34, 51, 51, 51, 353, 353, 353, 34, 51, 34, 51, 353, 353, 51, 571, 51, 51, 34, 34, 509, 34, 353, 353, 34, 51, 51, 34, 353, 34, 353, 34, 276, 51, 353, 353, 34, 34, 353, 353, 51, 51, 34, 51, 34, 353, 34, 34, 353, 34, 353, 34, 51, 353, 51, 353, 34, 353, 34, 353, 353, 34, 353, 353, 34, 51, 797, 51, 34, 51, 353, 353, 353, 353, 353, 34, 797, 34, 353, 353, 51, 571, 353, 34, 353, 353, 353, 51, 34, 51, 353, 51, 353, 34, 353, 571, 51, 353, 353, 51, 34, 51, 353, 51, 51, 51, 353, 353, 51, 353, 34, 34, 571, 276, 34, 34, 34, 797, 353, 353, 353, 353, 34, 34, 571, 34, 51, 797, 51, 34, 571, 34, 353, 353, 34, 51, 51, 34, 353, 353, 51, 353, 353, 51, 51, 353, 353, 34, 797, 51, 353, 353, 353, 571, 350, 353, 276, 34, 353, 34, 276, 51, 353, 51, 797, 353, 51, 353, 353, 353, 353, 34, 353, 353, 353, 51, 34, 34, 353, 353, 353, 276, 51, 353, 51, 51, 51, 353, 51, 34, 51, 34, 34, 51, 353, 34, 34, 353, 353, 353, 353, 571, 571, 276, 353, 571, 51, 353, 51, 51, 34, 34, 34, 353, 797, 276, 34, 350, 51, 34, 353, 51, 34, 797, 34, 51, 51, 51, 353, 571, 353, 51, 51, 353, 571, 34, 34, 797, 353, 51, 571, 34, 353, 353, 51, 34, 34, 51, 34, 51, 353, 276, 353, 353, 34, 353, 34, 353, 34, 353, 509, 353, 51, 353, 353, 34, 34, 353, 34, 353, 34, 34, 353, 51, 571, 797, 34, 353, 353, 34, 353, 353, 34, 51, 353, 353, 34, 276, 353, 51, 571, 34, 571, 34, 51, 571, 353, 571, 353, 34, 353, 353, 51, 34, 353, 353, 353, 353, 571, 51, 34, 51, 353, 34, 353, 571, 51, 51, 51, 353, 353, 797, 276, 51, 51, 353, 51, 51, 34, 34, 353, 797, 34, 797, 51, 353, 353, 34, 34, 353, 34, 353, 34, 34, 34, 353, 353, 353, 51, 51, 34, 34, 34, 51, 509, 34, 51, 34, 51, 353, 34, 51, 51, 276, 51, 353, 353, 51, 34, 353, 51, 353, 51, 51, 34, 353, 34, 51, 353, 353, 34, 51, 34, 571, 353, 51, 51, 353, 353, 353, 353, 353, 34, 34, 353, 34, 353, 34, 34, 34, 353, 353, 350, 353, 34, 51, 51, 353, 51, 353, 276, 353, 51, 353, 34, 51, 353, 353, 276, 34, 51, 34, 34, 34, 353, 353, 34, 34, 353, 353, 34, 51, 34, 34, 353, 353, 51, 51, 34, 51, 51, 51, 34, 353, 51, 353, 353, 34, 353, 353, 51, 51, 353, 51, 51, 51, 34, 51, 51, 353, 276, 51, 51, 34, 353, 34, 34, 34, 34, 51, 353, 353, 353, 34, 51, 34, 51, 353, 571, 51, 34, 51, 51, 51, 51, 353, 34, 571, 571, 353, 353, 34, 34, 51, 51, 353, 276, 34, 353, 34, 51, 34, 353, 34, 34, 276, 353, 51, 34, 51, 34, 34, 51, 34, 353, 353, 34, 51, 34, 571, 51, 51, 34, 51, 353, 353, 51, 34, 51, 276, 34, 51, 34, 276, 34, 51, 34, 34, 353, 51, 353, 34, 51, 51, 51, 353, 34, 571, 34, 353, 571, 51, 34, 34, 353, 51, 350, 51, 353, 353, 51, 51, 353, 353, 51, 350, 51, 51, 51, 353, 51, 509, 51, 51, 353, 353, 353, 34, 34, 353, 34, 51, 34, 51, 51, 34, 969, 34, 34, 276, 51, 353, 34, 34, 34, 34, 51, 51, 353, 350, 34, 353, 51, 353, 353, 34, 51, 34, 353, 51, 51, 353, 34, 509, 34, 34, 276, 51, 353, 51, 34, 51, 353, 353, 353, 51, 34, 34, 51, 34, 51, 353, 571, 353, 353, 353, 51, 34, 34, 350, 34, 353, 353, 353, 34, 34, 51, 34, 353, 34, 34, 353, 353, 34, 353, 353, 34, 797, 353, 34, 353, 353, 34, 353, 353, 51, 51, 34, 51, 34, 34, 51, 51, 353, 353, 34, 51, 51, 353, 51, 353, 51, 51, 51, 51, 797, 51, 51, 34, 51, 51, 34, 34, 34, 34, 34, 353, 353, 51, 34, 353, 353, 34, 51, 34, 51, 34, 353, 34, 34, 571, 34, 51, 353, 51, 353, 353, 353, 353, 353, 353, 797, 571, 34, 51, 51, 34, 34, 51, 797, 51, 34, 353, 34, 353, 353, 34, 51, 51, 34, 276, 353, 51, 51, 51, 51, 34, 350, 350, 353, 276, 797, 51, 353, 34, 51, 51, 353, 51, 353, 51, 51, 51, 353, 353, 34, 34, 353, 571, 51, 51, 34, 51, 353, 51, 353, 353, 353, 353, 353, 353, 353, 51, 353, 34, 34, 353, 353, 51, 34, 34, 353, 34, 797, 34, 353, 51, 34, 51, 51, 34, 34, 34, 509, 34, 51, 350, 34, 51, 51, 34, 51, 353, 34, 276, 51, 51, 34, 51, 353, 51, 353, 51, 51, 34, 51, 353, 51, 34, 51, 34, 353, 34, 51, 353, 353, 34, 34, 353, 51, 51, 353, 34, 34, 51, 51, 34, 353, 571, 34, 353, 34, 353, 353, 34, 353, 353, 51, 51, 51, 353, 34, 34, 34, 51, 51, 353, 276, 353, 34, 276, 51, 51, 353, 353, 34, 51, 51, 797, 797, 51, 353, 353, 34, 51, 51, 51, 353, 353, 51, 34, 353, 353, 353, 51, 276, 353, 51, 353, 353, 34, 51, 51, 353, 353, 353, 51, 51, 34, 34, 51, 353, 350, 51, 51, 51, 34, 353, 34, 353, 353, 51, 353, 353, 51, 276, 51, 353, 353, 51, 34, 353, 353, 797, 51, 34, 34, 353, 51, 350, 51, 51, 353, 353, 51, 34, 51, 34, 34, 353, 51, 353, 34, 34, 51, 34, 353, 51, 51, 353, 34, 51, 353, 51, 34, 353, 51, 353, 353, 51, 353, 51, 797, 34, 34, 51, 797, 51, 51, 34, 797, 353, 51, 353, 51, 353, 353, 353, 51, 571, 353, 51, 34, 353, 34, 34, 353, 353, 51, 353, 353, 353, 350, 353, 51, 51, 51, 51, 51, 353, 34, 353, 51, 353, 353, 353, 34, 797, 34, 353, 51, 353, 34, 353, 51, 353, 51, 34, 34, 353, 51, 51, 34, 353, 797, 51, 350, 51, 51, 51, 51, 34, 571, 51, 353, 34, 51, 353, 34, 51, 51, 353, 34, 353, 34, 353, 353, 353, 51, 353, 353, 353, 34, 353, 353, 34, 353, 353, 353, 34, 276, 51, 520, 51, 353, 34, 353, 34, 353, 34, 353, 34, 353, 276, 353, 51, 353, 353, 51, 353, 51, 353, 34, 51, 34, 353, 353, 797, 34, 353, 34, 51, 353, 797, 51, 51, 353, 34, 51, 34, 34, 34, 797, 51, 353, 353, 34, 353, 51, 353, 51, 51, 51, 51, 51, 51, 51, 353, 353, 51, 51, 34, 353, 353, 353, 34, 353, 34, 353, 797, 34, 34, 51, 353, 51, 509, 34, 51, 51, 51, 353, 51, 353, 353, 353, 571, 353, 51, 353, 51, 353, 51, 34, 34, 51, 353, 353, 34, 51, 353, 34, 276, 353, 51, 51, 34, 34, 353, 34, 51, 34, 51, 571, 34, 353, 34, 353, 353, 353, 51, 51, 34, 353, 51, 34, 353, 34, 571, 353, 353, 353, 51, 51, 797, 51, 353, 34, 34, 353, 353, 34, 34, 353, 51, 51, 51, 51, 353, 51, 353, 34, 34, 34, 797, 34, 34, 353, 51, 353, 51, 51, 34, 34, 51, 34, 353, 34, 571, 353, 34, 353, 51, 34, 34, 51, 51, 34, 34, 51, 353, 34, 34, 350, 34, 51, 353, 353, 797, 34, 51, 34, 34, 353, 353, 353, 51, 51, 353, 353, 51, 34, 353, 51, 353, 34, 353, 51, 51, 51, 353, 34, 34, 34, 353, 51, 34, 350, 797, 34, 51, 51, 353, 34, 34, 34, 34, 51, 51, 353, 34, 350, 350, 34, 51, 353, 51, 520, 34, 51, 51, 51, 353, 34, 51, 797, 51, 51, 51, 276, 353, 34, 353, 353, 276, 353, 353, 51, 34, 34, 353, 353, 34, 34, 51, 34, 51, 51, 51, 51, 51, 34, 34, 353, 51, 34, 51, 571, 34, 34, 51, 571, 51, 34, 51, 51, 353, 51, 51, 353, 51, 51, 353, 353, 51, 353, 353, 353, 51, 51, 353, 51, 353, 350, 34, 353, 51, 34, 34, 34, 51, 51, 51, 51, 353, 353, 34, 34, 353, 34, 51, 51, 51, 353, 353, 51, 276, 571, 353, 34, 51, 51, 353, 34, 350, 353, 353, 34, 34, 34, 353, 353, 34, 353, 51, 353, 34, 353, 34, 353, 51, 353, 51, 51, 353, 571, 34, 34, 797, 34, 353, 34, 34, 34, 51, 353, 353, 51, 51, 353, 34, 353, 51, 51, 34, 353, 571, 34, 34, 34, 34, 34, 34, 51, 353, 51, 51, 34, 353, 353, 51, 34, 571, 51, 34, 353, 353, 51, 51, 34, 353, 353, 51, 276, 353, 51, 353, 34, 51, 353, 350, 353, 353, 51, 353, 276, 34, 34, 51, 51, 353, 353, 797, 34, 353, 51, 51, 276, 34, 353, 51, 34, 353, 51, 51, 51, 34, 51, 51, 353, 34, 353, 353, 51, 51, 34, 353, 353, 51, 51, 51, 51, 34, 797, 51, 353, 353, 353, 353, 51, 51, 353, 34, 276, 353, 34, 276, 51, 34, 34, 51, 571, 51, 51, 353, 353, 353, 276, 51, 34, 353, 51, 51, 51, 51, 51, 350, 353, 51, 350, 353, 353, 353, 51, 51, 51, 51, 353, 51, 51, 34, 34, 51, 353, 353, 51, 51, 34, 51, 34, 51, 353, 51, 51, 353, 34, 353, 353, 34, 51, 353, 353, 51, 276, 34, 353, 353, 34, 353, 34, 353, 51, 51, 51, 353, 353, 51, 34, 34, 353, 34, 51, 353, 34, 34, 353, 34, 353, 34, 353, 51, 353, 51, 51, 34, 353, 51, 34, 34, 353, 353, 353, 51, 51, 51, 51, 509, 34, 51, 34, 34, 353, 51, 34, 353, 353, 353, 353, 353, 353, 353, 353, 353, 353, 34, 34, 34, 276, 353, 34, 51, 353, 353, 353, 353]. Check train.csv.\n",
            "Final predictions saved to C:\\wockd\\ultra\\ultra\\retina\\kia2.csv.\n"
          ]
        }
      ],
      "source": [
        "#또 오류 수정,,,\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "\n",
        "# Model creation function\n",
        "def create_model(model_name):\n",
        "    import timm\n",
        "    model = timm.create_model(model_name, pretrained=False)\n",
        "    model.eval()\n",
        "    return model\n",
        "\n",
        "# Prediction function\n",
        "def predict(model, data_loader, device):\n",
        "    preds = []\n",
        "    with torch.no_grad():\n",
        "        for images in tqdm(data_loader, desc=\"Predicting\"):\n",
        "            images = images.to(device)\n",
        "            outputs = model(images)\n",
        "            preds.append(torch.softmax(outputs, dim=1).cpu().numpy())\n",
        "    return np.concatenate(preds, axis=0)\n",
        "\n",
        "# Ensemble prediction function\n",
        "def ensemble_predictions(model_paths, model_name, test_loader, device):\n",
        "    ensemble_preds = []\n",
        "    for model_path in model_paths:\n",
        "        model = create_model(model_name).to(device)\n",
        "        state_dict = torch.load(model_path, map_location=device)\n",
        "        model.load_state_dict(state_dict, strict=False)\n",
        "\n",
        "        preds = predict(model, test_loader, device)\n",
        "        ensemble_preds.append(preds)\n",
        "\n",
        "    final_preds = np.mean(ensemble_preds, axis=0)\n",
        "    return final_preds\n",
        "\n",
        "# Settings\n",
        "model_paths = [\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\best_model_fold_0.pth\",\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\best_model_fold_1.pth\",\n",
        "    r\"C:\\wockd\\ultra\\ultra\\retina\\ckpt_kfold\\final_best_model.pth\"\n",
        "]\n",
        "model_name = \"deit3_small_patch16_224.fb_in22k_ft_in1k\"\n",
        "test_csv_path = r\"C:\\wockd\\ultra\\ultra\\retina\\test.csv\"\n",
        "train_csv_path = r\"C:\\wockd\\ultra\\ultra\\retina\\train.csv\"\n",
        "output_file = r\"C:\\wockd\\ultra\\ultra\\retina\\kia2.csv\"\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Label mapping\n",
        "train_df = pd.read_csv(train_csv_path, header=None, names=['img_path', 'upscale_img_path', 'class_name'])\n",
        "label_mapping = {i: class_name for i, class_name in enumerate(train_df['class_name'].unique())}\n",
        "\n",
        "# Check if missing labels exist\n",
        "print(\"Label mapping keys:\", label_mapping.keys())\n",
        "\n",
        "# Test data loading\n",
        "test_df = pd.read_csv(test_csv_path)\n",
        "\n",
        "# Dataset definition\n",
        "class CustomDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, img_paths, transforms=None):\n",
        "        self.img_paths = img_paths\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.img_paths)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img = Image.open(self.img_paths[idx]).convert(\"RGB\")\n",
        "        if self.transforms:\n",
        "            img = self.transforms(img)\n",
        "        return img\n",
        "\n",
        "# Image transformations\n",
        "test_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "# Test dataset and DataLoader\n",
        "test_dataset = CustomDataset(\n",
        "    img_paths=test_df['img_path'].values,\n",
        "    transforms=test_transform\n",
        ")\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, num_workers=0)\n",
        "\n",
        "# Perform ensemble\n",
        "final_predictions = ensemble_predictions(model_paths, model_name, test_loader, device)\n",
        "\n",
        "# Map predicted labels to class names with default handling\n",
        "final_labels = np.argmax(final_predictions, axis=1)\n",
        "final_class_names = [label_mapping.get(label, \"Unknown Class\") for label in final_labels]\n",
        "\n",
        "missing_labels = [label for label in final_labels if label not in label_mapping]\n",
        "if missing_labels:\n",
        "    print(f\"Warning: Missing mappings for labels {missing_labels}. Check train.csv.\")\n",
        "\n",
        "# Save results\n",
        "test_df['label'] = final_class_names\n",
        "test_df[['id', 'label']].to_csv(output_file, index=False)\n",
        "print(f\"Final predictions saved to {output_file}.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynzeOLcQQMNn"
      },
      "outputs": [],
      "source": [
        "# 각 K-Fold에서 생성된 best.pth 파일 경로 리스트\n",
        "model_paths = [\n",
        "    r\"C:\\\\wockd\\\\ultra\\\\ultra\\\\retina\\\\ckpt_kfold\\\\best_model_fold_0.pth\",\n",
        "    r\"C:\\\\wockd\\\\ultra\\\\ultra\\\\retina\\\\ckpt_kfold\\\\best_model_fold_1.pth\",\n",
        "    r\"C:\\\\wockd\\\\ultra\\\\ultra\\\\retina\\\\ckpt_kfold\\\\final_best_model.pth\"\n",
        "]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "huaIs5Udp6M_"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "ckpt_df = pd.DataFrame({'fname':glob('./ckpt/*.ckpt')})\n",
        "ckpt_df['mtime'] = ckpt_df.fname.apply(lambda x: int(os.stat(x).st_mtime))\n",
        "ckpt_df['model_name'] = ckpt_df.fname.apply(lambda x: re.search(r'./ckpt/(.*?)-fold',x)[1])\n",
        "ckpt_df['img_size'] = ckpt_df.fname.apply(lambda x: int(re.search(r'patch[0-9]+_([0-9]+)', x + 'patch0_0')[1]) )\n",
        "ckpt_df['is_ema'] = ckpt_df.fname.str.endswith('ema.ckpt').astype(int)\n",
        "ckpt_df['fold_idx'] = ckpt_df.fname.apply(lambda x: int(re.search(r'fold_idx=([0-9])-',x)[1]))\n",
        "ckpt_df['val_loss'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_loss=(0\\.[0-9]+)', x)[1]) )\n",
        "ckpt_df['val_score'] = ckpt_df.fname.apply(lambda x: float(re.search(r'val_score=(0\\.[0-9]+)', x)[1]) )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35zxQc25p6NA"
      },
      "outputs": [],
      "source": [
        "ckpt_df = ckpt_df[ckpt_df.img_size != 0][ckpt_df.is_ema == 0]\n",
        "ckpt_df = ckpt_df.sort_values('mtime',ascending=False).reset_index(drop=True)\n",
        "ckpt_indexes = ckpt_df[ ckpt_df.fold_idx==ckpt_df.fold_idx.max() ].index[:4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytuv1jwKZKbK"
      },
      "outputs": [],
      "source": [
        "print(ckpt_df)\n",
        "print(ckpt_indexes )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LBYZdFP7xbRM"
      },
      "outputs": [],
      "source": [
        "print(\"CFG['IMG_SIZE']:\", CFG['IMG_SIZE'])\n",
        "assert CFG['IMG_SIZE'] in (192, 224, 336,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 694,
          "referenced_widgets": [
            "aa00a5b26fad4176aa6b83e7856d2684",
            "a14ae28f81da47659cda2b2d4961cc7a",
            "3787ca47d0ff4af7bdc1b8aac5674cbe",
            "9c2df1ce600944e2b922fea4143dee71",
            "7a302a9db13d48698b4006acd6e5eb1b",
            "b3d42b5016d04fa98bed4e5bd4324a1f",
            "bb81241ad9df4174a25ca6c5c1cf3330",
            "591a5143345043f18db442a8042d036b",
            "fb9e654d7cba46fcb00736302df9a0ca",
            "bd8b67478e8249a78fb0ad5b2404ae2e",
            "246a1d862234473982eb4c8755b7f43f"
          ]
        },
        "id": "62GWW8lYc62S",
        "outputId": "dd8a36db-b0e7-41e7-b251-b26a678f425b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11/19 08:22:35 [INFO] ./ckpt/deit3_large_patch16_224.fb_in22k_ft_in1k-fold_idx=0-epoch=08-val_loss=0.4284-val_score=0.9783.ckpt loading\n",
            "11/19 08:22:35 [INFO] 224\n",
            "11/19 08:22:35 [INFO] load_img_size=224\n",
            "11/19 08:22:35 [INFO] create_model: deit3_large_patch16_224.fb_in22k_ft_in1k\n",
            "11/19 08:22:35 [INFO] Loading pretrained weights from Hugging Face hub (timm/deit3_large_patch16_224.fb_in22k_ft_in1k)\n",
            "11/19 08:22:35 [INFO] [timm/deit3_large_patch16_224.fb_in22k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
            "11/19 08:22:40 [INFO] ./ckpt/deit3_large_patch16_224.fb_in22k_ft_in1k-fold_idx=0-epoch=08-val_loss=0.4284-val_score=0.9783.ckpt loading\n",
            "<ipython-input-28-132539c7501f>:27: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
            "  model.load_state_dict( torch.load(checkpoint_path)['model'] )\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa00a5b26fad4176aa6b83e7856d2684",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/71 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "11/19 08:41:34 [ERRO] -------Exception----------\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/IPython/core/interactiveshell.py\", line 3553, in run_code\n",
            "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
            "  File \"<ipython-input-28-132539c7501f>\", line 30, in <cell line: 6>\n",
            "    preds.append( prediction(model, test_loader, device) )\n",
            "  File \"<ipython-input-20-5c04b002d755>\", line 8, in prediction\n",
            "    for batch in tqdm(test_loader):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/notebook.py\", line 250, in __iter__\n",
            "    for obj in it:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/tqdm/std.py\", line 1181, in __iter__\n",
            "    for obj in iterable:\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 701, in __next__\n",
            "    data = self._next_data()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py\", line 757, in _next_data\n",
            "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
            "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
            "  File \"<ipython-input-11-02b6361a4f46>\", line 50, in __getitem__\n",
            "    image = self.get_image_from_index( index, self.load_img_size )\n",
            "  File \"<ipython-input-11-02b6361a4f46>\", line 45, in get_image_from_index\n",
            "    img = self.get_interpolated_image(Image.open(img_path), img_size )\n",
            "  File \"<ipython-input-11-02b6361a4f46>\", line 30, in get_interpolated_image\n",
            "    from wand import image\n",
            "ModuleNotFoundError: No module named 'wand'\n"
          ]
        }
      ],
      "source": [
        "#from PIL import Image\n",
        "\n",
        "preds = []\n",
        "preds_score = []\n",
        "\n",
        "for ckpt_start_index in ckpt_indexes:\n",
        "    logger.info(f'{ckpt_df.fname[ckpt_start_index]} loading')\n",
        "    ## imagesize\n",
        "    CFG['IMG_SIZE'] = ckpt_df.img_size[ckpt_start_index]\n",
        "    assert CFG['IMG_SIZE'] in ( 196, 224, )\n",
        "    logger.info(CFG['IMG_SIZE'])\n",
        "\n",
        "    test_dataset = CustomDataset(\n",
        "        test_df['img_path'].values, None,\n",
        "        interpolation=CFG['INTERPOLATION'], load_img_size=CFG['IMG_SIZE'],\n",
        "        shuffle=False, transforms=test_transform)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=CFG['BATCH_SIZE']*2, shuffle=False, num_workers=0)\n",
        "\n",
        "    model_name = ckpt_df.model_name[ckpt_start_index]\n",
        "    model = create_model(model_name)\n",
        "    if ckpt_df.is_ema[ckpt_start_index]:\n",
        "        model = torch.optim.swa_utils.AveragedModel(model)\n",
        "    #-----------------------------\n",
        "    for i in range(ckpt_start_index, ckpt_start_index + ckpt_df.fold_idx.max() + 1 ):\n",
        "        checkpoint_path = ckpt_df.fname[i]\n",
        "        logger.info(f'{checkpoint_path} loading')\n",
        "        model.load_state_dict( torch.load(checkpoint_path)['model'] )\n",
        "\n",
        "        preds_score.append( ckpt_df.val_score[i] )\n",
        "        preds.append( prediction(model, test_loader, device) )\n",
        "\n",
        "preds = np.array(preds)\n",
        "preds_score = np.array(preds_score)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6YKTBPSGvE7N"
      },
      "outputs": [],
      "source": [
        "# ### 가중치 평균값..\n",
        "preds_error = (1-preds_score)  ## L1 ACC 오차인경우\n",
        "preds_error = 1-preds_error/preds_error.sum()\n",
        "preds_coef = preds_error/preds_error.sum()\n",
        "\n",
        "logger.info(f'{preds_score=}')\n",
        "logger.info(f'{preds_coef=}')\n",
        "preds2 = np.array( [ coef * preds[i] for i, coef in enumerate( preds_coef ) ] )\n",
        "preds_labels = le.inverse_transform(preds2.sum(0).argmax(-1))\n",
        "print(preds_labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J36qWyQ_vIRY"
      },
      "outputs": [],
      "source": [
        "submit = pd.read_csv('./sample_submission.csv')\n",
        "submit['label'] = preds_labels\n",
        "from datetime import datetime\n",
        "dt_str = datetime.now().strftime('%Y%m%d_%H%M')\n",
        "submit.to_csv(f'./basslibrary_submit_{dt_str}.csv', index=False)\n",
        "logger.info(f'./basslibrary_submit_{dt_str}.csv saved')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XleqiuwyvKch"
      },
      "outputs": [],
      "source": [
        "submit.label.value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RZo9oMpITxaP"
      },
      "outputs": [],
      "source": [
        "# !python ~/send_telegram.py 'basslibrary_submit_{dt_str}.csv saved'"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "newwoc",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.20"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "246a1d862234473982eb4c8755b7f43f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3787ca47d0ff4af7bdc1b8aac5674cbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "danger",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_591a5143345043f18db442a8042d036b",
            "max": 71,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_fb9e654d7cba46fcb00736302df9a0ca",
            "value": 44
          }
        },
        "591a5143345043f18db442a8042d036b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a302a9db13d48698b4006acd6e5eb1b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9c2df1ce600944e2b922fea4143dee71": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_bd8b67478e8249a78fb0ad5b2404ae2e",
            "placeholder": "​",
            "style": "IPY_MODEL_246a1d862234473982eb4c8755b7f43f",
            "value": " 44/71 [18:50&lt;12:54, 28.70s/it]"
          }
        },
        "a14ae28f81da47659cda2b2d4961cc7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b3d42b5016d04fa98bed4e5bd4324a1f",
            "placeholder": "​",
            "style": "IPY_MODEL_bb81241ad9df4174a25ca6c5c1cf3330",
            "value": " 62%"
          }
        },
        "aa00a5b26fad4176aa6b83e7856d2684": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_a14ae28f81da47659cda2b2d4961cc7a",
              "IPY_MODEL_3787ca47d0ff4af7bdc1b8aac5674cbe",
              "IPY_MODEL_9c2df1ce600944e2b922fea4143dee71"
            ],
            "layout": "IPY_MODEL_7a302a9db13d48698b4006acd6e5eb1b"
          }
        },
        "b3d42b5016d04fa98bed4e5bd4324a1f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bb81241ad9df4174a25ca6c5c1cf3330": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "bd8b67478e8249a78fb0ad5b2404ae2e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb9e654d7cba46fcb00736302df9a0ca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}